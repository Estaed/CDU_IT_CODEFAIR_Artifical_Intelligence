{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kriol → English NMT (NLLB, single-GPU)\n",
        "\n",
        "This notebook trains a Kriol → English translator using NLLB (facebook/nllb-200-distilled-600M) with the Hugging Face Trainer.\n",
        "\n",
        "- Cleans and preprocesses pairs, caching a cleaned CSV to speed reruns\n",
        "- Trains a single-GPU baseline and saves a `final/` checkpoint with HF artifacts and `.pth`\n",
        "- Optional: back-translation plan and custom tokenizer scaffolding (placeholders)\n",
        "\n",
        "References:\n",
        "- NLLB model card: https://huggingface.co/facebook/nllb-200-distilled-600M\n",
        "- Transformers Seq2Seq docs: https://huggingface.co/docs/transformers/en/tasks/translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 — Environment & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0+cu129\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer as Trainer,\n",
        "    Seq2SeqTrainingArguments as TrainingArguments,\n",
        ")\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 — Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CFG:\n",
        "    # Model & paths\n",
        "    MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "    OUTPUT_DIR = \"../model/\"\n",
        "    CFG_JSON = \"CFG.json\"\n",
        "\n",
        "    # Data\n",
        "    DATA_FILE = \"../data/train_data.xlsx\"\n",
        "    CLEAN_DATA_FILE = \"../data/train_data_cleaned.csv\"\n",
        "    SRC_COL = \"kriol\"\n",
        "    TGT_COL = \"english\"\n",
        "    VAL_SIZE = 0.1\n",
        "    SEED = 42\n",
        "\n",
        "    # NLLB language tags\n",
        "    SRC_LANG = None  # e.g., \"eng_Latn\" if source were English; set properly when defined\n",
        "    TGT_LANG = \"eng_Latn\"\n",
        "\n",
        "    # Preprocessing\n",
        "    APPLY_ENGLISH_LID = True\n",
        "    MAX_TOKENS = 128\n",
        "    LEN_RATIO = 3.0\n",
        "    STRIP_PUNCT_SRC = True\n",
        "    STRIP_PUNCT_TGT = False\n",
        "\n",
        "    # Cleaning control\n",
        "    SKIP_CLEAN_IF_EXISTS = True\n",
        "\n",
        "    # Cross-validation\n",
        "    USE_CV = False\n",
        "    K_FOLDS = 5\n",
        "    CV_FOLD = 0\n",
        "\n",
        "    # Training\n",
        "    NUM_EPOCHS = 12\n",
        "    BATCH_SIZE = 8\n",
        "    LR = 5e-5\n",
        "    MAX_LEN = 128\n",
        "    EVAL_EVERY_STEPS = 2000  # step-based eval printing frequency\n",
        "\n",
        "    # Decoding\n",
        "    BEAM_SIZE = 6\n",
        "    LENGTH_PENALTY = 1.0\n",
        "    EARLY_STOPPING = True\n",
        "\n",
        "    GRADIENT_CHECKPOINTING = True\n",
        "\n",
        "    # Back-translation\n",
        "    ENABLE_BT = False\n",
        "    EN2KR_MODEL = \"Helsinki-NLP/opus-mt-en-mul\"\n",
        "    EN2KR_DIR = \"../model/en2kriol\"\n",
        "    SYNTH_CSV = \"../data/synthetic/en_to_kriol_v1.csv\"\n",
        "    SYNTH_CSV_SAMPLE = \"../data/synthetic/en_to_kriol_v1_sample.csv\"\n",
        "    BT_FAST = False\n",
        "    BT_BEAM_SIZE = 1  # will be adjusted below\n",
        "    BT_LENGTH_PENALTY = 1.0\n",
        "    BT_EARLY_STOPPING = True\n",
        "    BT_BATCH = 64\n",
        "\n",
        "    # Synthetic integration\n",
        "    INTEGRATE_SYNTH = False\n",
        "    SYNTH_MAX_RATIO = 1.0\n",
        "\n",
        "    # Tokenizer (custom placeholder)\n",
        "    USE_SPM = False\n",
        "    SPM_DIR = \"../outputs/tokenizers/spm_kriol_en_v1\"\n",
        "\n",
        "    # Decoding/generation extras\n",
        "    GEN_MAX_NEW_TOKENS = 64\n",
        "\n",
        "    # COMET\n",
        "    COMET_MODEL = \"Unbabel/wmt22-comet-da\"\n",
        "    COMET_BATCH = 16\n",
        "\n",
        "    # Trainer args\n",
        "    WARMUP_STEPS = 500\n",
        "    GRAD_ACCUM_STEPS = 2\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "    LOGGING_STEPS = 50\n",
        "    SAVE_STEPS = 1000\n",
        "    SAVE_TOTAL_LIMIT = 3\n",
        "    FP16 = True\n",
        "    REPORT_TO = [\"tensorboard\"]\n",
        "\n",
        "    # Augmented training\n",
        "    RETRAIN_WITH_SYNTH = False\n",
        "    AUG_OUTPUT_DIR = \"../model/final_aug\"\n",
        "\n",
        "# dynamic ties\n",
        "CFG.BT_BEAM_SIZE = 1 if CFG.BT_FAST else 4\n",
        "\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save CFG to JSON for reproducibility\n",
        "try:\n",
        "    import json\n",
        "    cfg_path = os.path.join(CFG.OUTPUT_DIR, \"CFG.json\")\n",
        "    cfg_dict = {k: getattr(CFG, k) for k in dir(CFG) if k.isupper()}\n",
        "    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cfg_dict, f, indent=2)\n",
        "except Exception as _e:\n",
        "    print(\"CFG save failed:\", _e)\n",
        "\n",
        "torch.manual_seed(CFG.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(CFG.SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 — Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prefer cleaned CSV if present; otherwise load raw and proceed to Step 4\n",
        "if os.path.exists(CFG.CLEAN_DATA_FILE):\n",
        "    df = pd.read_csv(CFG.CLEAN_DATA_FILE)\n",
        "else:\n",
        "    if CFG.DATA_FILE.endswith(\".csv\"):\n",
        "        df = pd.read_csv(CFG.DATA_FILE)\n",
        "    elif CFG.DATA_FILE.endswith((\".xlsx\", \".xls\")):\n",
        "        df = pd.read_excel(CFG.DATA_FILE)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported data file format: use .csv or .xlsx\")\n",
        "\n",
        "assert CFG.SRC_COL in df.columns and CFG.TGT_COL in df.columns, f\"Columns not found: {CFG.SRC_COL}, {CFG.TGT_COL}\"\n",
        "\n",
        "# Keep minimal shape only; Step 4 handles full cleaning\n",
        "df = df[[CFG.SRC_COL, CFG.TGT_COL]].dropna()\n",
        "df = df[(df[CFG.SRC_COL].astype(str).str.strip() != \"\") & (df[CFG.TGT_COL].astype(str).str.strip() != \"\")]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 4 — Clean and persist dataset (merged cleaner + execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing cleaned dataset: ../data/train_data_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "\n",
        "try:\n",
        "    import langid as _langid\n",
        "except Exception:\n",
        "    _langid = None\n",
        "\n",
        "\n",
        "def _to_str(x):\n",
        "    return \"\" if x is None else str(x)\n",
        "\n",
        "\n",
        "def _normalize_unicode(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = _to_str(s)\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = re.sub(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069\\uFEFF]\", \"\", s)\n",
        "    s = \"\".join(ch for ch in s if (ch in \"\\t\\n\\r\" or unicodedata.category(ch)[0] != \"C\"))\n",
        "    return s\n",
        "\n",
        "\n",
        "def _fix_mojibake(s: str) -> str:\n",
        "    replacements = {\n",
        "        \"â€™\": \"'\", \"â€˜\": \"'\", \"â€œ\": '\"', \"â€�\": '\"',\n",
        "        \"â€“\": \"-\", \"â€”\": \"-\", \"Â \": \" \", \"Â \": \" \",\n",
        "    }\n",
        "    for k, v in replacements.items():\n",
        "        s = s.replace(k, v)\n",
        "    return html.unescape(s)\n",
        "\n",
        "\n",
        "def _normalize_quotes_and_spacing_no_punct_space(s: str) -> str:\n",
        "    # Standardize quotes/dashes and collapse spaces; do not add spaces around punctuation here\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def _strip_punct(s: str) -> str:\n",
        "    return re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "\n",
        "\n",
        "def _simple_tok_count(s: str) -> int:\n",
        "    return 0 if not s else len(s.split())\n",
        "\n",
        "\n",
        "def _kriol_token_score(s: str) -> int:\n",
        "    kriol_markers = {\"bin\",\"langa\",\"blanga\",\"det\",\"im\",\"imbin\",\"garra\",\"olabat\",\"nomo\",\"wal\",\"mob\",\"deya\",\n",
        "    \"garram\",\"gin\",\"seim\"}\n",
        "    tokens = set(_to_str(s).lower().split())\n",
        "    return sum(1 for t in tokens if t in kriol_markers)\n",
        "\n",
        "\n",
        "def _maybe_swap(src: str, tgt: str, use_langid: bool = True):\n",
        "    kriol_src = _kriol_token_score(src)\n",
        "    kriol_tgt = _kriol_token_score(tgt)\n",
        "    en_src = en_tgt = 0\n",
        "    if use_langid and _langid is not None:\n",
        "        try:\n",
        "            en_src = 1 if _langid.classify(_to_str(src))[0] == \"en\" else 0\n",
        "            en_tgt = 1 if _langid.classify(_to_str(tgt))[0] == \"en\" else 0\n",
        "        except Exception:\n",
        "            pass\n",
        "    should_swap = (en_src > en_tgt and kriol_tgt > kriol_src and (kriol_tgt - kriol_src) >= 1)\n",
        "    return (tgt, src) if should_swap else (src, tgt)\n",
        "\n",
        "\n",
        "def clean_parallel_dataframe(\n",
        "    df,\n",
        "    src_col: str,\n",
        "    tgt_col: str,\n",
        "    lowercase_src: bool = True,\n",
        "    lowercase_tgt: bool = False,\n",
        "    strip_punct_src: bool = True,\n",
        "    strip_punct_tgt: bool = False,\n",
        "    max_tokens: int = 128,\n",
        "    len_ratio: float = 3.0,\n",
        "    apply_english_lid_on_tgt: bool = False,\n",
        "    try_swap_misplaced_rows: bool = True,\n",
        "    drop_identical_pairs: bool = True,\n",
        "):\n",
        "    work = df[[src_col, tgt_col]].copy()\n",
        "\n",
        "    for c in (src_col, tgt_col):\n",
        "        work[c] = (\n",
        "            work[c]\n",
        "            .astype(str)\n",
        "            .map(_normalize_unicode)\n",
        "            .map(_fix_mojibake)\n",
        "            .map(_normalize_quotes_and_spacing_no_punct_space)\n",
        "        )\n",
        "\n",
        "    if try_swap_misplaced_rows:\n",
        "        work[[src_col, tgt_col]] = work.apply(\n",
        "            lambda r: _maybe_swap(r[src_col], r[tgt_col], use_langid=True), axis=1, result_type=\"expand\"\n",
        "        )\n",
        "\n",
        "    if lowercase_src:\n",
        "        work[src_col] = work[src_col].str.lower()\n",
        "    if lowercase_tgt:\n",
        "        work[tgt_col] = work[tgt_col].str.lower()\n",
        "    if strip_punct_src:\n",
        "        work[src_col] = work[src_col].map(_strip_punct)\n",
        "    if strip_punct_tgt:\n",
        "        work[tgt_col] = work[tgt_col].map(_strip_punct)\n",
        "\n",
        "    work = work[(work[src_col].str.strip() != \"\") & (work[tgt_col].str.strip() != \"\")]\n",
        "    if drop_identical_pairs:\n",
        "        work = work[work[src_col] != work[tgt_col]]\n",
        "\n",
        "    work = work.drop_duplicates(subset=[src_col, tgt_col])\n",
        "\n",
        "    def _keep_len(row) -> bool:\n",
        "        s_len = _simple_tok_count(row[src_col])\n",
        "        t_len = _simple_tok_count(row[tgt_col])\n",
        "        if s_len == 0 or t_len == 0:\n",
        "            return False\n",
        "        if s_len > max_tokens or t_len > max_tokens:\n",
        "            return False\n",
        "        ratio = max(s_len / max(1, t_len), t_len / max(1, s_len))\n",
        "        return ratio <= len_ratio\n",
        "\n",
        "    work = work[work.apply(_keep_len, axis=1)]\n",
        "\n",
        "    if apply_english_lid_on_tgt and _langid is not None:\n",
        "        try:\n",
        "            work = work[work[tgt_col].map(lambda s: _langid.classify(_to_str(s))[0] == \"en\")]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return work.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Execute cleaning and persist (skippable when cleaned exists)\n",
        "if CFG.SKIP_CLEAN_IF_EXISTS and os.path.exists(CFG.CLEAN_DATA_FILE):\n",
        "    print(f\"Using existing cleaned dataset: {CFG.CLEAN_DATA_FILE}\")\n",
        "    df = pd.read_csv(CFG.CLEAN_DATA_FILE)\n",
        "else:\n",
        "    cleaned = clean_parallel_dataframe(\n",
        "        df,\n",
        "        src_col=CFG.SRC_COL,\n",
        "        tgt_col=CFG.TGT_COL,\n",
        "        lowercase_src=True,\n",
        "        lowercase_tgt=False,\n",
        "        strip_punct_src=CFG.STRIP_PUNCT_SRC,\n",
        "        strip_punct_tgt=CFG.STRIP_PUNCT_TGT,\n",
        "        max_tokens=CFG.MAX_TOKENS,\n",
        "        len_ratio=CFG.LEN_RATIO,\n",
        "        apply_english_lid_on_tgt=CFG.APPLY_ENGLISH_LID,\n",
        "        try_swap_misplaced_rows=True,\n",
        "    )\n",
        "\n",
        "    os.makedirs(os.path.dirname(CFG.CLEAN_DATA_FILE), exist_ok=True)\n",
        "    cleaned.to_csv(CFG.CLEAN_DATA_FILE, index=False)\n",
        "    print(f\"Saved cleaned dataset: {CFG.CLEAN_DATA_FILE} rows={len(cleaned)}\")\n",
        "\n",
        "    # Use cleaned data downstream\n",
        "    df = cleaned.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 — Preprocess & normalize\n",
        "\n",
        "This step prepares pairs for Kriol→English only:\n",
        "- Lowercase both sides; normalize whitespace\n",
        "- Deduplicate pairs to avoid train/val leakage\n",
        "- Length filter: max 128 tokens on each side\n",
        "- Length ratio filter: src/tgt and tgt/src ≤ 3.0\n",
        "- Optional: language ID filter for English targets (off by default)\n",
        "\n",
        "Note: These filters run before the split to ensure clean train/val sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filters: 20424 2270\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import langid\n",
        "except Exception:\n",
        "    langid = None\n",
        "\n",
        "\n",
        "def _normalize_common(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = re.sub(r\"\\s*([,;:?!\\.])\\s*\", r\" \\1 \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def _strip_punct(s: str) -> str:\n",
        "    # remove common punctuation; keep alphanumerics and spaces\n",
        "    return re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "\n",
        "def normalize_src_text(s: str) -> str:\n",
        "    s = _normalize_common(s)\n",
        "    if CFG.STRIP_PUNCT_SRC:\n",
        "        s = _strip_punct(s)\n",
        "    return s.lower()\n",
        "\n",
        "def normalize_tgt_text(s: str) -> str:\n",
        "    s = _normalize_common(s)\n",
        "    if CFG.STRIP_PUNCT_TGT:\n",
        "        s = _strip_punct(s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def simple_token_count(text: str) -> int:\n",
        "    return len(str(text).split())\n",
        "\n",
        "\n",
        "def passes_filters(row) -> bool:\n",
        "    src = normalize_src_text(row[CFG.SRC_COL])\n",
        "    tgt = normalize_tgt_text(row[CFG.TGT_COL])\n",
        "    if src == \"\" or tgt == \"\":\n",
        "        return False\n",
        "    # length tokens\n",
        "    s_len = simple_token_count(src)\n",
        "    t_len = simple_token_count(tgt)\n",
        "    if s_len > CFG.MAX_TOKENS or t_len > CFG.MAX_TOKENS:\n",
        "        return False\n",
        "    # ratio\n",
        "    if s_len > 0 and t_len > 0:\n",
        "        if s_len / t_len > CFG.LEN_RATIO or t_len / s_len > CFG.LEN_RATIO:\n",
        "            return False\n",
        "    # optional English LID on target\n",
        "    if CFG.APPLY_ENGLISH_LID and langid is not None:\n",
        "        lid, _ = langid.classify(tgt)\n",
        "        if lid != \"en\":\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Apply normalization + filters before split\n",
        "df_filtered = df[[CFG.SRC_COL, CFG.TGT_COL]].dropna().copy()\n",
        "df_filtered[CFG.SRC_COL] = df_filtered[CFG.SRC_COL].apply(normalize_src_text)\n",
        "df_filtered[CFG.TGT_COL] = df_filtered[CFG.TGT_COL].apply(normalize_tgt_text)\n",
        "df_filtered = df_filtered.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "df_filtered = df_filtered[df_filtered.apply(passes_filters, axis=1)]\n",
        "\n",
        "# Split: K-Fold when enabled, else single random split\n",
        "if CFG.USE_CV:\n",
        "    assert CFG.K_FOLDS >= 2, \"K_FOLDS must be >= 2 for cross-validation\"\n",
        "    kf = KFold(n_splits=CFG.K_FOLDS, shuffle=True, random_state=CFG.SEED)\n",
        "    folds = list(kf.split(df_filtered))\n",
        "    fold_idx = CFG.CV_FOLD % CFG.K_FOLDS\n",
        "    train_index, val_index = folds[fold_idx]\n",
        "    train_df = df_filtered.iloc[train_index].reset_index(drop=True)\n",
        "    val_df = df_filtered.iloc[val_index].reset_index(drop=True)\n",
        "    print(f\"KFold split: fold {fold_idx+1}/{CFG.K_FOLDS} -> train {len(train_df)} val {len(val_df)}\")\n",
        "else:\n",
        "    train_df, val_df = train_test_split(df_filtered, test_size=CFG.VAL_SIZE, random_state=CFG.SEED)\n",
        "    print(\"After filters:\", len(train_df), len(val_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6 — Back-translation plan (design, no-op)\n",
        "\n",
        "Goal: augment Kriol→English training with synthetic pairs generated by an English→Kriol reverse model.\n",
        "\n",
        "Plan:\n",
        "- Train/load reverse MarianMT (en→kriol) with same normalization rules.\n",
        "- Generate synthetic Kriol for English monolingual (start with our training English).\n",
        "- Build synthetic pairs (kriol_syn, english_orig), dedup + filter, and merge with real pairs using sample weighting.\n",
        "- Re-train forward model and evaluate COMET.\n",
        "\n",
        "Artifacts:\n",
        "- Reverse model at `model/en2kriol/`\n",
        "- Synthetic CSV at `data/synthetic/en_to_kriol_v1.csv`\n",
        "\n",
        "Note: This cell does not execute generation; code scaffold follows below and is disabled by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Back-translation scaffold (enable with CFG.ENABLE_BT)\n",
        "\n",
        "def maybe_load_en2kriol():\n",
        "    tok = AutoTokenizer.from_pretrained(CFG.EN2KR_MODEL)\n",
        "    mdl = AutoModelForSeq2SeqLM.from_pretrained(CFG.EN2KR_MODEL).to(device)\n",
        "    return tok, mdl\n",
        "\n",
        "@torch.no_grad()\n",
        "def en_to_kriol_generate(tok, mdl, texts: List[str]) -> List[str]:\n",
        "    mdl.eval()\n",
        "    inputs = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    out = mdl.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=CFG.GEN_MAX_NEW_TOKENS,\n",
        "        num_beams=CFG.BT_BEAM_SIZE,\n",
        "        length_penalty=CFG.BT_LENGTH_PENALTY,\n",
        "        early_stopping=CFG.BT_EARLY_STOPPING,\n",
        "    )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "# Generate and save synthetic data when enabled\n",
        "if CFG.ENABLE_BT:\n",
        "    from math import ceil\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "    except Exception:\n",
        "        tqdm = None\n",
        "\n",
        "    tok_r, mdl_r = maybe_load_en2kriol()\n",
        "    eng_all = train_df[CFG.TGT_COL].tolist()\n",
        "    BATCH_BT = CFG.BT_BATCH\n",
        "    kriol_syn_all: List[str] = []\n",
        "\n",
        "    total = len(eng_all)\n",
        "    it = range(0, total, BATCH_BT)\n",
        "    if tqdm is not None:\n",
        "        it = tqdm(it, total=ceil(total / BATCH_BT), desc=\"Back-translation\")\n",
        "\n",
        "    for i in it:\n",
        "        batch = eng_all[i:i+BATCH_BT]\n",
        "        kriol_syn_all.extend(en_to_kriol_generate(tok_r, mdl_r, batch))\n",
        "\n",
        "    os.makedirs(os.path.dirname(CFG.SYNTH_CSV), exist_ok=True)\n",
        "    synth = pd.DataFrame({CFG.SRC_COL: kriol_syn_all, CFG.TGT_COL: eng_all})\n",
        "    synth.to_csv(CFG.SYNTH_CSV, index=False)\n",
        "\n",
        "    # Save a small preview sample for manual QA (e.g., 100 rows)\n",
        "    try:\n",
        "        n_preview = min(100, len(synth))\n",
        "        synth.head(n_preview).to_csv(CFG.SYNTH_CSV_SAMPLE, index=False)\n",
        "    except Exception as _e:\n",
        "        print(\"Could not save synthetic preview:\", _e)\n",
        "\n",
        "    print(f\"Saved synthetic pairs: {CFG.SYNTH_CSV} rows: {len(synth)} (preview: {CFG.SYNTH_CSV_SAMPLE})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚠️ DO NOT RUN NOW (design only)\n",
        "\n",
        "### Step 7 — Tokenizer training plan (no-op)\n",
        "\n",
        "- Objective: Prepare a shared SentencePiece tokenizer plan for Kriol↔English without changing current training.\n",
        "- Corpus: All training sentences from both Kriol and English sides combined.\n",
        "- Preprocessing: lowercase, normalize whitespace/punctuation, keep dialectal spellings, drop empties, deduplicate.\n",
        "- Hyperparameters: vocab_size=8000 (up to 12000 if OOV>1.5%), character_coverage=0.9995, model_type=unigram; special tokens: <pad>, <s>, </s>, <unk>.\n",
        "- Training flags (indicative):\n",
        "  - --model_type=unigram --vocab_size=8000 --character_coverage=0.9995\n",
        "  - --shuffle_input_sentence=true --max_sentence_length=2048 --num_threads=[CPU cores]\n",
        "- Artifacts: spm_kriol_en_v1.model, spm_kriol_en_v1.vocab + a JSON with training config.\n",
        "- Adoption criteria: Switch only if corpus grows >30% or OOV >1.5% and A/B shows COMET improvement.\n",
        "\n",
        "Evaluation protocol (A/B): Train/score with Marian default vs SentencePiece on same split; report COMET delta and per-segment examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer scaffolding (no-op; does not alter training unless enabled)\n",
        "USE_SPM = CFG.USE_SPM\n",
        "SPM_DIR = CFG.SPM_DIR  # where spm.model/spm.vocab would live\n",
        "\n",
        "\n",
        "def prepare_tokenizer_corpus(df, src_col: str, tgt_col: str):\n",
        "    \"\"\"Return a list of cleaned lines for SPM training (lowercase, strip, dedup).\"\"\"\n",
        "    src = df[src_col].astype(str).str.lower().str.strip()\n",
        "    tgt = df[tgt_col].astype(str).str.lower().str.strip()\n",
        "    lines = pd.concat([src, tgt], ignore_index=True)\n",
        "    lines = lines[lines != \"\"].drop_duplicates()\n",
        "    return lines.tolist()\n",
        "\n",
        "\n",
        "def train_sentencepiece_corpus(lines, model_prefix: str, vocab_size: int = 8000):\n",
        "    \"\"\"Sketch only: real training will be added later. No side effects here.\"\"\"\n",
        "    # import sentencepiece as spm\n",
        "    # spm.SentencePieceTrainer.Train(\n",
        "    #     input=data_path,\n",
        "    #     model_prefix=model_prefix,\n",
        "    #     vocab_size=vocab_size,\n",
        "    #     character_coverage=0.9995,\n",
        "    #     model_type=\"unigram\",\n",
        "    # )\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_tokenizer(marian_model_name: str, use_spm: bool = USE_SPM):\n",
        "    \"\"\"Return tokenizer, preferring SPM dir if enabled, else Marian default.\"\"\"\n",
        "    if use_spm and os.path.isdir(SPM_DIR):\n",
        "        return AutoTokenizer.from_pretrained(SPM_DIR)\n",
        "    return AutoTokenizer.from_pretrained(marian_model_name)\n",
        "\n",
        "# Note: current notebook flow continues to use Marian tokenizer by default.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8 — Generate synthetic data (optional)\n",
        "Creates English→Kriol synthetic pairs using a reverse model when enabled in Step 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9 — Integrate synthetic data (optional)\n",
        "Applies the same normalization & filtering rules as real data, dedups, and preserves real-only validation (train-only merge).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10 — Custom tokenizer training (not finished)\n",
        "Trains a shared SentencePiece tokenizer on combined Kriol+English corpus when enabled in Step 2. This is different from the default Marian tokenizer. It’s only adopted if metrics improve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10.1 — Custom tokenizer (placeholder)\n",
        "\n",
        "(Empty for now — we will implement a custom tokenizer later.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic integration OFF or CSV not found at ../data/synthetic/en_to_kriol_v1.csv. Skipping integration.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "SYNTH_CSV_PATH = CFG.SYNTH_CSV\n",
        "\n",
        "try:\n",
        "    if CFG.INTEGRATE_SYNTH and os.path.exists(SYNTH_CSV_PATH):\n",
        "        syn_raw = pd.read_csv(SYNTH_CSV_PATH)\n",
        "        if not {CFG.SRC_COL, CFG.TGT_COL}.issubset(set(syn_raw.columns)):\n",
        "            raise ValueError(f\"Synthetic CSV missing required columns: {CFG.SRC_COL}, {CFG.TGT_COL}\")\n",
        "\n",
        "        # Normalize + filter synthetic with same functions\n",
        "        syn_df = syn_raw[[CFG.SRC_COL, CFG.TGT_COL]].dropna().copy()\n",
        "        syn_df[CFG.SRC_COL] = syn_df[CFG.SRC_COL].apply(normalize_src_text)\n",
        "        syn_df[CFG.TGT_COL] = syn_df[CFG.TGT_COL].apply(normalize_tgt_text)\n",
        "        syn_df = syn_df.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "        syn_df = syn_df[syn_df.apply(passes_filters, axis=1)]\n",
        "\n",
        "        # Merge synthetic into training only; keep validation purely real\n",
        "        before_train = len(train_df)\n",
        "        max_synth = int(CFG.SYNTH_MAX_RATIO * before_train)\n",
        "        if len(syn_df) > max_synth:\n",
        "            syn_df = syn_df.sample(n=max_synth, random_state=CFG.SEED)\n",
        "\n",
        "        train_df = pd.concat([train_df, syn_df], ignore_index=True)\n",
        "        train_df = train_df.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "        after_train = len(train_df)\n",
        "\n",
        "        print(\n",
        "            f\"Synthetic integration complete: +{after_train - before_train} pairs (train: {after_train}, val: {len(val_df)})\"\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Synthetic integration OFF or CSV not found at {SYNTH_CSV_PATH}. Skipping integration.\")\n",
        "except Exception as e:\n",
        "    print(\"Synthetic integration error:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11 — Tokenizer & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[warn] CFG.SRC_LANG is unset. Using fallback src_lang=eng_Latn. Set CFG.SRC_LANG explicitly when decided.\n",
            "cuda NVIDIA GeForce RTX 5060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# NLLB requires language codes for tokenizer; set with safe fallback\n",
        "src_lang = CFG.SRC_LANG if CFG.SRC_LANG else \"eng_Latn\"\n",
        "tgt_lang = CFG.TGT_LANG if CFG.TGT_LANG else \"eng_Latn\"\n",
        "if CFG.SRC_LANG is None:\n",
        "    print(\"[warn] CFG.SRC_LANG is unset. Using fallback src_lang=eng_Latn. Set CFG.SRC_LANG explicitly when decided.\")\n",
        "\n",
        "# Load tokenizer/model (8-bit quantization removed)\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.MODEL_NAME)\n",
        "# Disable cache during training to avoid decoder arg conflicts\n",
        "if hasattr(model, \"config\"):\n",
        "    model.config.use_cache = False\n",
        "\n",
        "# Ensure decoder start and BOS/EOS tokens are set for NLLB/M2M100\n",
        "if hasattr(model, \"config\") and hasattr(tokenizer, \"lang_code_to_id\"):\n",
        "    forced_bos = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    if forced_bos is not None and forced_bos != tokenizer.unk_token_id:\n",
        "        model.config.forced_bos_token_id = forced_bos\n",
        "    if getattr(model.config, \"decoder_start_token_id\", None) is None:\n",
        "        model.config.decoder_start_token_id = model.config.forced_bos_token_id\n",
        "\n",
        "# Gradient checkpointing for VRAM (disable to avoid rare HF decoder arg issues)\n",
        "if False and CFG.GRADIENT_CHECKPOINTING and hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is not available. Please check your GPU drivers and PyTorch install.\"\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(device, torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12 — Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20424, 2270)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class PairedTextDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int):\n",
        "        self.src = df[CFG.SRC_COL].tolist()\n",
        "        self.tgt = df[CFG.TGT_COL].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        src_text = str(self.src[idx])\n",
        "        tgt_text = str(self.tgt[idx])\n",
        "        model_inputs = self.tokenizer(\n",
        "            src_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        labels = self.tokenizer(\n",
        "            text_target=tgt_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
        "        item[\"labels\"] = labels[\"input_ids\"].squeeze(0)\n",
        "        return item\n",
        "\n",
        "train_ds = PairedTextDataset(train_df, tokenizer, CFG.MAX_LEN)\n",
        "val_ds = PairedTextDataset(val_df, tokenizer, CFG.MAX_LEN)\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 13 — Trainer setup (DDP-ready)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: Trainer that drops unintended *_embeds keys to avoid HF arg conflicts\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "class CleanSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def _prepare_inputs(self, inputs):\n",
        "        # Sanitize at input-prep stage too\n",
        "        inputs.pop(\"decoder_inputs_embeds\", None)\n",
        "        inputs.pop(\"inputs_embeds\", None)\n",
        "        inputs.pop(\"decoder_input_ids\", None)\n",
        "        allowed = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"}\n",
        "        filtered = {k: v for k, v in inputs.items() if k in allowed}\n",
        "        return super()._prepare_inputs(filtered)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Drop any embed keys and strictly whitelist safe args for seq2seq training\n",
        "        inputs.pop(\"decoder_inputs_embeds\", None)\n",
        "        inputs.pop(\"inputs_embeds\", None)\n",
        "        inputs.pop(\"decoder_input_ids\", None)\n",
        "        allowed = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"}\n",
        "        filtered = {k: v for k, v in inputs.items() if k in allowed}\n",
        "        # Call model explicitly with safe kwargs to avoid decoder ids/embeds conflicts\n",
        "        outputs = model(\n",
        "            decoder_input_ids=None,\n",
        "            decoder_inputs_embeds=None,\n",
        "            use_cache=False,\n",
        "            **filtered,\n",
        "        )\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\TARIK\\AppData\\Local\\Temp\\ipykernel_23620\\1757983301.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CleanSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = CleanSeq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "label_pad_token_id = -100\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=label_pad_token_id, padding=True)\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    num_train_epochs=CFG.NUM_EPOCHS,\n",
        "    per_device_train_batch_size=CFG.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=CFG.BATCH_SIZE,\n",
        "    learning_rate=CFG.LR,\n",
        "    warmup_steps=CFG.WARMUP_STEPS,\n",
        "    gradient_accumulation_steps=CFG.GRAD_ACCUM_STEPS,\n",
        "    label_smoothing_factor=CFG.LABEL_SMOOTHING,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    save_total_limit=CFG.SAVE_TOTAL_LIMIT,\n",
        "    fp16=CFG.FP16,\n",
        "    report_to=CFG.REPORT_TO,\n",
        "    eval_accumulation_steps=1,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        ")\n",
        "\n",
        "trainer = CleanSeq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 14 — Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='15324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    9/15324 01:11 < 43:35:12, 0.10 it/s, Epoch 0.01/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Evaluate once to log metrics\u001b[39;00m\n\u001b[32m      4\u001b[39m metrics = trainer.evaluate()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:2623\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2621\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2622\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2625\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2626\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:5581\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5579\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5580\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5582\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5583\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\accelerate\\data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mPairedTextDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     12\u001b[39m src_text = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.src[idx])\n\u001b[32m     13\u001b[39m tgt_text = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.tgt[idx])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m labels = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m     22\u001b[39m     text_target=tgt_text,\n\u001b[32m     23\u001b[39m     max_length=\u001b[38;5;28mself\u001b[39m.max_len,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m item = {k: v.squeeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_inputs.items()}\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2911\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2909\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2910\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2911\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2913\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3021\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   3000\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3001\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3018\u001b[39m         **kwargs,\n\u001b[32m   3019\u001b[39m     )\n\u001b[32m   3020\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3021\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3096\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3067\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3068\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3069\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3084\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3087\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3088\u001b[39m     padding=padding,\n\u001b[32m   3089\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3093\u001b[39m     **kwargs,\n\u001b[32m   3094\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:627\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    604\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    605\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     **kwargs,\n\u001b[32m    625\u001b[39m ) -> BatchEncoding:\n\u001b[32m    626\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:553\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    565\u001b[39m tokens_and_encodings = [\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    567\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    577\u001b[39m ]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate once to log metrics\n",
        "metrics = trainer.evaluate()\n",
        "print(\"eval_loss:\", metrics.get(\"eval_loss\"))\n",
        "\n",
        "# Launch TensorBoard from notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"../model\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save HF artifacts and a .pth checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "trainer.save_model(final_dir)\n",
        "model_path = os.path.join(final_dir, \"model_state.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Saved .pth to: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 15 — Inference helper (final only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "\n",
        "final_tok = AutoTokenizer.from_pretrained(final_dir, src_lang=(CFG.SRC_LANG or \"eng_Latn\"), tgt_lang=(CFG.TGT_LANG or \"eng_Latn\"))\n",
        "final_model = AutoModelForSeq2SeqLM.from_pretrained(final_dir).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with(model, tok, texts):\n",
        "    inputs = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    forced_bos = tok.convert_tokens_to_ids(CFG.TGT_LANG or \"eng_Latn\")\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=CFG.GEN_MAX_NEW_TOKENS,\n",
        "        num_beams=CFG.BEAM_SIZE,\n",
        "        length_penalty=CFG.LENGTH_PENALTY,\n",
        "        early_stopping=CFG.EARLY_STOPPING,\n",
        "        forced_bos_token_id=forced_bos,\n",
        "    )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "# Pick 3 random samples from training data and show Kriol / predicted / original\n",
        "rows = val_df.sample(n=3)\n",
        "kriols = rows[CFG.SRC_COL].astype(str).tolist()\n",
        "eng_refs = rows[CFG.TGT_COL].astype(str).tolist()\n",
        "eng_preds = generate_with(final_model, final_tok, kriols)\n",
        "\n",
        "for i, (kriol, pred, ref) in enumerate(zip(kriols, eng_preds, eng_refs), start=1):\n",
        "    print(f\"Sample {i}\")\n",
        "    print(\"Kriol:\", kriol)\n",
        "    print(\"English predicted:\", pred)\n",
        "    print(\"English original:\", ref)\n",
        "    print(\"-\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 16 — Retrain with synthetic data (optional)\n",
        "If Step 8 integrated synthetic pairs and CFG.RETRAIN_WITH_SYNTH=True, reinitialize datasets and run a second training to produce `final_aug/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional retraining on merged dataset\n",
        "if CFG.RETRAIN_WITH_SYNTH and CFG.INTEGRATE_SYNTH:\n",
        "    # Rebuild datasets from potentially expanded train_df\n",
        "    train_ds_aug = PairedTextDataset(train_df, tokenizer, CFG.MAX_LEN)\n",
        "    collator_aug = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=-100, padding=True)\n",
        "\n",
        "    args_aug = TrainingArguments(\n",
        "        output_dir=CFG.AUG_OUTPUT_DIR,\n",
        "        num_train_epochs=CFG.NUM_EPOCHS,\n",
        "        per_device_train_batch_size=CFG.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=CFG.BATCH_SIZE,\n",
        "        learning_rate=CFG.LR,\n",
        "        warmup_steps=CFG.WARMUP_STEPS,\n",
        "        gradient_accumulation_steps=CFG.GRAD_ACCUM_STEPS,\n",
        "        label_smoothing_factor=CFG.LABEL_SMOOTHING,\n",
        "        optim=\"adamw_torch\",\n",
        "        logging_steps=CFG.LOGGING_STEPS,\n",
        "        save_steps=CFG.SAVE_STEPS,\n",
        "        save_total_limit=CFG.SAVE_TOTAL_LIMIT,\n",
        "        fp16=CFG.FP16,\n",
        "        report_to=CFG.REPORT_TO,\n",
        "        remove_unused_columns=False,\n",
        "        label_names=[\"labels\"],\n",
        "    )\n",
        "\n",
        "    trainer_aug = Trainer(\n",
        "        model=model,\n",
        "        args=args_aug,\n",
        "        train_dataset=train_ds_aug,\n",
        "        eval_dataset=val_df,  # keep same real-only val\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator_aug,\n",
        "    )\n",
        "\n",
        "    trainer_aug.train()\n",
        "\n",
        "    # Save augmented final\n",
        "    os.makedirs(CFG.AUG_OUTPUT_DIR, exist_ok=True)\n",
        "    trainer_aug.save_model(CFG.AUG_OUTPUT_DIR)\n",
        "    torch.save(model.state_dict(), os.path.join(CFG.AUG_OUTPUT_DIR, \"model_state.pth\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 17 — COMET evaluation (final only)\n",
        "Scores validation translations with Unbabel COMET if available; otherwise prints a note (Python 3.13 may lack wheels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMET: evaluate final only\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "\n",
        "    BATCH = CFG.COMET_BATCH\n",
        "    refs = val_df[CFG.TGT_COL].tolist()\n",
        "    srcs = val_df[CFG.SRC_COL].tolist()\n",
        "\n",
        "    def batched_hyps_final():\n",
        "        hyps = []\n",
        "        for i in range(0, len(val_df), BATCH):\n",
        "            hyps.extend(generate_with(final_model, final_tok, srcs[i:i+BATCH]))\n",
        "        return hyps\n",
        "\n",
        "    hyps_final = batched_hyps_final()\n",
        "    data_final = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(srcs, hyps_final, refs)]\n",
        "\n",
        "    model_path = download_model(CFG.COMET_MODEL)\n",
        "    comet_model = load_from_checkpoint(model_path)\n",
        "\n",
        "    def get_score(output):\n",
        "        if isinstance(output, dict):\n",
        "            return output.get(\"system_score\") or output.get(\"score\") or output.get(\"mean_score\")\n",
        "        try:\n",
        "            _, s = output\n",
        "            return s\n",
        "        except Exception:\n",
        "            return output\n",
        "\n",
        "    def get_segments(output):\n",
        "        if isinstance(output, dict):\n",
        "            segs = output.get(\"segments\") or output.get(\"scores\") or output.get(\"segment_scores\")\n",
        "            if isinstance(segs, list):\n",
        "                return segs\n",
        "        return None\n",
        "\n",
        "    out_final = comet_model.predict(data_final, batch_size=BATCH, gpus=1 if torch.cuda.is_available() else 0)\n",
        "\n",
        "    sf = get_score(out_final)\n",
        "    try:\n",
        "        print(\"COMET (final):\", f\"{float(sf):.4f}\")\n",
        "    except Exception:\n",
        "        print(\"COMET (final, raw):\", sf)\n",
        "\n",
        "    # Save COMET outputs to disk\n",
        "    final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    def safe_float(x):\n",
        "        try:\n",
        "            return float(x)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Write system score\n",
        "    sf_f = safe_float(sf)\n",
        "    try:\n",
        "        with open(os.path.join(final_dir, \"system_score.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sf_f if sf_f is not None else sf}\\n\")\n",
        "    except Exception as _e:\n",
        "        print(\"Could not save final system score:\", _e)\n",
        "\n",
        "    # Write per-segment CSV (src, mt, ref, score)\n",
        "    seg_final = get_segments(out_final)\n",
        "\n",
        "    try:\n",
        "        if isinstance(seg_final, list) and len(seg_final) == len(hyps_final):\n",
        "            df_final = pd.DataFrame({\n",
        "                \"src\": srcs,\n",
        "                \"mt\": hyps_final,\n",
        "                \"ref\": refs,\n",
        "                \"comet_score\": seg_final,\n",
        "            })\n",
        "            df_final.to_csv(os.path.join(final_dir, \"comet_segments.csv\"), index=False)\n",
        "    except Exception as _e:\n",
        "        print(\"Could not save final segments CSV:\", _e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"COMET evaluation unavailable:\", e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
