{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kriol → English NMT\n",
        "\n",
        "This notebook trains a Kriol → English translator using NLLB (facebook/nllb-200-distilled-600M) with the Hugging Face Trainer.\n",
        "\n",
        "- Cleans and preprocesses pairs, caching a cleaned CSV to speed reruns\n",
        "- Trains a single-GPU baseline and saves a `final/` checkpoint with HF artifacts and `.pth`\n",
        "- Optional: back-translation plan and custom tokenizer scaffolding (placeholders)\n",
        "\n",
        "References:\n",
        "- NLLB model card: https://huggingface.co/facebook/nllb-200-distilled-600M\n",
        "- Transformers Seq2Seq docs: https://huggingface.co/docs/transformers/en/tasks/translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 — Environment & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0+cu129\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer as Trainer,\n",
        "    Seq2SeqTrainingArguments as TrainingArguments,\n",
        ")\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 — Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CFG:\n",
        "    # Model & paths\n",
        "    MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "    OUTPUT_DIR = \"../model/\"\n",
        "\n",
        "    # Data\n",
        "    DATA_FILE = \"../data/train_data.xlsx\"\n",
        "    CLEAN_DATA_FILE = \"../data/train_data_cleaned.csv\"\n",
        "    SRC_COL = \"kriol\"\n",
        "    TGT_COL = \"english\"\n",
        "    VAL_SIZE = 0.2\n",
        "    SEED = 42\n",
        "\n",
        "    # NLLB language tags (proxy Kriol as Tok Pisin for tokenizer)\n",
        "    SRC_LANG = \"tpi_Latn\"\n",
        "    TGT_LANG = \"eng_Latn\"\n",
        "\n",
        "    # Preprocessing\n",
        "    APPLY_ENGLISH_LID = True\n",
        "    MAX_TOKENS = 128\n",
        "    LEN_RATIO = 3.0\n",
        "    STRIP_PUNCT_SRC = False\n",
        "    STRIP_PUNCT_TGT = False\n",
        "\n",
        "    # Cleaning control\n",
        "    SKIP_CLEAN_IF_EXISTS = True\n",
        "\n",
        "    # Cross-validation\n",
        "    USE_CV = True\n",
        "    K_FOLDS = 5\n",
        "    CV_FOLD = 0\n",
        "\n",
        "    # Training\n",
        "    NUM_EPOCHS = 15\n",
        "    BATCH_SIZE = 8\n",
        "    LR = 3e-5\n",
        "    MAX_LEN = 128\n",
        "    DROPOUT = 0.2\n",
        "    ATTENTION_DROPOUT = 0.1\n",
        "\n",
        "    # Decoding\n",
        "    BEAM_SIZE = 6\n",
        "    LENGTH_PENALTY = 1.0\n",
        "    EARLY_STOPPING = True\n",
        "\n",
        "    # Back-translation\n",
        "    ENABLE_BT = True\n",
        "    # Use NLLB to generate Kriol-proxy via Tok Pisin (tpi) from English\n",
        "    EN2KR_MODEL = \"facebook/nllb-200-distilled-600M\"\n",
        "    EN2KR_SRC_LANG = \"eng_Latn\"\n",
        "    EN2KR_TGT_LANG = \"tpi_Latn\"\n",
        "    SYNTH_CSV = \"../data/synthetic/en_to_kriol_v1.csv\"\n",
        "    SYNTH_CSV_SAMPLE = \"../data/synthetic/en_to_kriol_v1_sample.csv\"\n",
        "    BT_FAST = False\n",
        "    BT_BEAM_SIZE = 8\n",
        "    BT_LENGTH_PENALTY = 1.1\n",
        "    BT_EARLY_STOPPING = True\n",
        "    BT_BATCH = 64\n",
        "\n",
        "    # Back-translation decode controls\n",
        "    BT_NUM_BEST = 3\n",
        "    BT_MIN_LENGTH = 6\n",
        "    BT_REPETITION_PENALTY = 1.1\n",
        "\n",
        "    # Synthetic integration\n",
        "    SYNTH_MAX_RATIO = 1.0\n",
        "\n",
        "    # Tokenizer (custom placeholder)\n",
        "    USE_SPM = True\n",
        "    SPM_DIR = \"../outputs/tokenizers/spm_kriol_en_v1\"\n",
        "\n",
        "    # Decoding/generation extras\n",
        "    GEN_MAX_NEW_TOKENS = 32\n",
        "\n",
        "    # COMET\n",
        "    COMET_MODEL = \"Unbabel/wmt22-comet-da\"\n",
        "    COMET_BATCH = 32\n",
        "\n",
        "    # Trainer args\n",
        "    WARMUP_STEPS = 500\n",
        "    GRAD_ACCUM_STEPS = 2\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "    LOGGING_STEPS = 50\n",
        "    SAVE_STEPS = 1000\n",
        "    SAVE_TOTAL_LIMIT = 3\n",
        "    FP16 = True\n",
        "    REPORT_TO = [\"tensorboard\"]\n",
        "\n",
        "    DROPOUT = 0.2\n",
        "    ATTENTION_DROPOUT = 0.1\n",
        "    \n",
        "    # Increase these for better results\n",
        "    WARMUP_RATIO = 0.1  # Better warmup\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    \n",
        "    # Mixed precision training\n",
        "    BF16 = torch.cuda.is_bf16_supported()  # Use BF16 if available\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    LR_SCHEDULER = \"cosine\"\n",
        "    \n",
        "    # Gradient clipping\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "\n",
        "\n",
        "# dynamic ties\n",
        "CFG.BT_BEAM_SIZE = 1 if CFG.BT_FAST else 8\n",
        "\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(CFG.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(CFG.SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 — Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prefer cleaned CSV if present; otherwise load raw and proceed to Step 4\n",
        "if os.path.exists(CFG.CLEAN_DATA_FILE):\n",
        "    df = pd.read_csv(CFG.CLEAN_DATA_FILE)\n",
        "else:\n",
        "    if CFG.DATA_FILE.endswith(\".csv\"):\n",
        "        df = pd.read_csv(CFG.DATA_FILE)\n",
        "    elif CFG.DATA_FILE.endswith((\".xlsx\", \".xls\")):\n",
        "        df = pd.read_excel(CFG.DATA_FILE)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported data file format: use .csv or .xlsx\")\n",
        "\n",
        "assert CFG.SRC_COL in df.columns and CFG.TGT_COL in df.columns, f\"Columns not found: {CFG.SRC_COL}, {CFG.TGT_COL}\"\n",
        "\n",
        "# Keep minimal shape only; Step 4 handles full cleaning\n",
        "df = df[[CFG.SRC_COL, CFG.TGT_COL]].dropna()\n",
        "df = df[(df[CFG.SRC_COL].astype(str).str.strip() != \"\") & (df[CFG.TGT_COL].astype(str).str.strip() != \"\")]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 4 — Clean and persist dataset (merged cleaner + execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing cleaned dataset: ../data/train_data_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "\n",
        "try:\n",
        "    import langid as _langid\n",
        "except Exception:\n",
        "    _langid = None\n",
        "\n",
        "\n",
        "def _to_str(x):\n",
        "    return \"\" if x is None else str(x)\n",
        "\n",
        "\n",
        "def _normalize_unicode(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = _to_str(s)\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = re.sub(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069\\uFEFF]\", \"\", s)\n",
        "    s = \"\".join(ch for ch in s if (ch in \"\\t\\n\\r\" or unicodedata.category(ch)[0] != \"C\"))\n",
        "    return s\n",
        "\n",
        "\n",
        "def _fix_mojibake(s: str) -> str:\n",
        "    replacements = {\n",
        "        \"â€™\": \"'\", \"â€˜\": \"'\", \"â€œ\": '\"', \"â€�\": '\"',\n",
        "        \"â€“\": \"-\", \"â€”\": \"-\", \"Â \": \" \", \"Â \": \" \",\n",
        "    }\n",
        "    for k, v in replacements.items():\n",
        "        s = s.replace(k, v)\n",
        "    return html.unescape(s)\n",
        "\n",
        "\n",
        "def _normalize_quotes_and_spacing_no_punct_space(s: str) -> str:\n",
        "    # Standardize quotes/dashes and collapse spaces; do not add spaces around punctuation here\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def _strip_punct(s: str) -> str:\n",
        "    return re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "\n",
        "\n",
        "def _simple_tok_count(s: str) -> int:\n",
        "    return 0 if not s else len(s.split())\n",
        "\n",
        "\n",
        "def _kriol_token_score(s: str) -> int:\n",
        "    kriol_markers = {\"bin\",\"langa\",\"blanga\",\"det\",\"im\",\"imbin\",\"garra\",\"olabat\",\"nomo\",\"wal\",\"mob\",\"deya\",\n",
        "    \"garram\",\"gin\",\"seim\"}\n",
        "    tokens = set(_to_str(s).lower().split())\n",
        "    return sum(1 for t in tokens if t in kriol_markers)\n",
        "\n",
        "\n",
        "def _maybe_swap(src: str, tgt: str, use_langid: bool = True):\n",
        "    kriol_src = _kriol_token_score(src)\n",
        "    kriol_tgt = _kriol_token_score(tgt)\n",
        "    en_src = en_tgt = 0\n",
        "    if use_langid and _langid is not None:\n",
        "        try:\n",
        "            en_src = 1 if _langid.classify(_to_str(src))[0] == \"en\" else 0\n",
        "            en_tgt = 1 if _langid.classify(_to_str(tgt))[0] == \"en\" else 0\n",
        "        except Exception:\n",
        "            pass\n",
        "    should_swap = (en_src > en_tgt and kriol_tgt > kriol_src and (kriol_tgt - kriol_src) >= 1)\n",
        "    return (tgt, src) if should_swap else (src, tgt)\n",
        "\n",
        "\n",
        "def clean_parallel_dataframe(\n",
        "    df,\n",
        "    src_col: str,\n",
        "    tgt_col: str,\n",
        "    lowercase_src: bool = True,\n",
        "    lowercase_tgt: bool = False,\n",
        "    strip_punct_src: bool = True,\n",
        "    strip_punct_tgt: bool = False,\n",
        "    max_tokens: int = 128,\n",
        "    len_ratio: float = 3.0,\n",
        "    apply_english_lid_on_tgt: bool = False,\n",
        "    try_swap_misplaced_rows: bool = True,\n",
        "    drop_identical_pairs: bool = True,\n",
        "):\n",
        "    work = df[[src_col, tgt_col]].copy()\n",
        "\n",
        "    for c in (src_col, tgt_col):\n",
        "        work[c] = (\n",
        "            work[c]\n",
        "            .astype(str)\n",
        "            .map(_normalize_unicode)\n",
        "            .map(_fix_mojibake)\n",
        "            .map(_normalize_quotes_and_spacing_no_punct_space)\n",
        "        )\n",
        "\n",
        "    if try_swap_misplaced_rows:\n",
        "        work[[src_col, tgt_col]] = work.apply(\n",
        "            lambda r: _maybe_swap(r[src_col], r[tgt_col], use_langid=True), axis=1, result_type=\"expand\"\n",
        "        )\n",
        "\n",
        "    if lowercase_src:\n",
        "        work[src_col] = work[src_col].str.lower()\n",
        "    if lowercase_tgt:\n",
        "        work[tgt_col] = work[tgt_col].str.lower()\n",
        "    if strip_punct_src:\n",
        "        work[src_col] = work[src_col].map(_strip_punct)\n",
        "    if strip_punct_tgt:\n",
        "        work[tgt_col] = work[tgt_col].map(_strip_punct)\n",
        "\n",
        "    work = work[(work[src_col].str.strip() != \"\") & (work[tgt_col].str.strip() != \"\")]\n",
        "    if drop_identical_pairs:\n",
        "        work = work[work[src_col] != work[tgt_col]]\n",
        "\n",
        "    work = work.drop_duplicates(subset=[src_col, tgt_col])\n",
        "\n",
        "    def _keep_len(row) -> bool:\n",
        "        s_len = _simple_tok_count(row[src_col])\n",
        "        t_len = _simple_tok_count(row[tgt_col])\n",
        "        if s_len == 0 or t_len == 0:\n",
        "            return False\n",
        "        if s_len > max_tokens or t_len > max_tokens:\n",
        "            return False\n",
        "        ratio = max(s_len / max(1, t_len), t_len / max(1, s_len))\n",
        "        return ratio <= len_ratio\n",
        "\n",
        "    work = work[work.apply(_keep_len, axis=1)]\n",
        "\n",
        "    if apply_english_lid_on_tgt and _langid is not None:\n",
        "        try:\n",
        "            work = work[work[tgt_col].map(lambda s: _langid.classify(_to_str(s))[0] == \"en\")]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return work.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Execute cleaning and persist (skippable when cleaned exists)\n",
        "if CFG.SKIP_CLEAN_IF_EXISTS and os.path.exists(CFG.CLEAN_DATA_FILE):\n",
        "    print(f\"Using existing cleaned dataset: {CFG.CLEAN_DATA_FILE}\")\n",
        "    df = pd.read_csv(CFG.CLEAN_DATA_FILE)\n",
        "else:\n",
        "    cleaned = clean_parallel_dataframe(\n",
        "        df,\n",
        "        src_col=CFG.SRC_COL,\n",
        "        tgt_col=CFG.TGT_COL,\n",
        "        lowercase_src=True,\n",
        "        lowercase_tgt=False,\n",
        "        strip_punct_src=CFG.STRIP_PUNCT_SRC,\n",
        "        strip_punct_tgt=CFG.STRIP_PUNCT_TGT,\n",
        "        max_tokens=CFG.MAX_TOKENS,\n",
        "        len_ratio=CFG.LEN_RATIO,\n",
        "        apply_english_lid_on_tgt=CFG.APPLY_ENGLISH_LID,\n",
        "        try_swap_misplaced_rows=True,\n",
        "    )\n",
        "\n",
        "    os.makedirs(os.path.dirname(CFG.CLEAN_DATA_FILE), exist_ok=True)\n",
        "    cleaned.to_csv(CFG.CLEAN_DATA_FILE, index=False)\n",
        "    print(f\"Saved cleaned dataset: {CFG.CLEAN_DATA_FILE} rows={len(cleaned)}\")\n",
        "\n",
        "    # Use cleaned data downstream\n",
        "    df = cleaned.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 — Preprocess & normalize\n",
        "\n",
        "This step prepares pairs for Kriol→English only:\n",
        "- Lowercase both sides; normalize whitespace\n",
        "- Deduplicate pairs to avoid train/val leakage\n",
        "- Length filter: max 128 tokens on each side\n",
        "- Length ratio filter: src/tgt and tgt/src ≤ 3.0\n",
        "- Optional: language ID filter for English targets (off by default)\n",
        "\n",
        "Note: These filters run before the split to ensure clean train/val sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KFold split: fold 1/5 -> train 18155 val 4539\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import langid\n",
        "except Exception:\n",
        "    langid = None\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def _normalize_common(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = re.sub(r\"\\s*([,;:?!\\.])\\s*\", r\" \\1 \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def _strip_punct(s: str) -> str:\n",
        "    # remove common punctuation; keep alphanumerics and spaces\n",
        "    return re.sub(r\"[^\\w\\s]\", \"\", s)\n",
        "\n",
        "def normalize_src_text(s: str) -> str:\n",
        "    s = _normalize_common(s)\n",
        "    if CFG.STRIP_PUNCT_SRC:\n",
        "        s = _strip_punct(s)\n",
        "    return s.lower()\n",
        "\n",
        "def normalize_tgt_text(s: str) -> str:\n",
        "    s = _normalize_common(s)\n",
        "    if CFG.STRIP_PUNCT_TGT:\n",
        "        s = _strip_punct(s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def simple_token_count(text: str) -> int:\n",
        "    return len(str(text).split())\n",
        "\n",
        "\n",
        "def passes_filters(row) -> bool:\n",
        "    src = normalize_src_text(row[CFG.SRC_COL])\n",
        "    tgt = normalize_tgt_text(row[CFG.TGT_COL])\n",
        "    if src == \"\" or tgt == \"\":\n",
        "        return False\n",
        "    # length tokens\n",
        "    s_len = simple_token_count(src)\n",
        "    t_len = simple_token_count(tgt)\n",
        "    if s_len > CFG.MAX_TOKENS or t_len > CFG.MAX_TOKENS:\n",
        "        return False\n",
        "    # ratio\n",
        "    if s_len > 0 and t_len > 0:\n",
        "        if s_len / t_len > CFG.LEN_RATIO or t_len / s_len > CFG.LEN_RATIO:\n",
        "            return False\n",
        "    # optional English LID on target\n",
        "    if CFG.APPLY_ENGLISH_LID and langid is not None:\n",
        "        lid, _ = langid.classify(tgt)\n",
        "        if lid != \"en\":\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Apply normalization + filters before split\n",
        "df_filtered = df[[CFG.SRC_COL, CFG.TGT_COL]].dropna().copy()\n",
        "df_filtered[CFG.SRC_COL] = df_filtered[CFG.SRC_COL].apply(normalize_src_text)\n",
        "df_filtered[CFG.TGT_COL] = df_filtered[CFG.TGT_COL].apply(normalize_tgt_text)\n",
        "df_filtered = df_filtered.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "df_filtered = df_filtered[df_filtered.apply(passes_filters, axis=1)]\n",
        "\n",
        "# Split: K-Fold when enabled, else single random split\n",
        "if CFG.USE_CV:\n",
        "    assert CFG.K_FOLDS >= 2, \"K_FOLDS must be >= 2 for cross-validation\"\n",
        "    kf = KFold(n_splits=CFG.K_FOLDS, shuffle=True, random_state=CFG.SEED)\n",
        "    folds = list(kf.split(df_filtered))\n",
        "    fold_idx = CFG.CV_FOLD % CFG.K_FOLDS\n",
        "    train_index, val_index = folds[fold_idx]\n",
        "    train_df = df_filtered.iloc[train_index].reset_index(drop=True)\n",
        "    val_df = df_filtered.iloc[val_index].reset_index(drop=True)\n",
        "    print(f\"KFold split: fold {fold_idx+1}/{CFG.K_FOLDS} -> train {len(train_df)} val {len(val_df)}\")\n",
        "else:\n",
        "    train_df, val_df = train_test_split(df_filtered, test_size=CFG.VAL_SIZE, random_state=CFG.SEED)\n",
        "    print(\"After filters:\", len(train_df), len(val_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6 — Data augmentation (applied to training only)\n",
        "\n",
        "Applies light noise to Kriol sources and optional synonym replacement on English targets for a small subset of the training set. Validation data is never augmented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\TARIK\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmented +-932 examples (train=21762, val=4539)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import wordnet as wn\n",
        "    try:\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "except Exception:\n",
        "    nltk = None\n",
        "    wn = None\n",
        "\n",
        "random.seed(CFG.SEED)\n",
        "\n",
        "def inject_noise(sentence: str, drop_p: float = 0.05, swap_p: float = 0.05) -> str:\n",
        "    toks = sentence.split()\n",
        "    # random drop\n",
        "    keep = []\n",
        "    for t in toks:\n",
        "        if random.random() > drop_p:\n",
        "            keep.append(t)\n",
        "    toks = keep if keep else toks\n",
        "    # random swap (one pair)\n",
        "    if len(toks) >= 2 and random.random() < swap_p:\n",
        "        i = random.randrange(0, len(toks)-1)\n",
        "        toks[i], toks[i+1] = toks[i+1], toks[i]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "\n",
        "def synonym_replace_en(sentence: str, prob: float = 0.1) -> str:\n",
        "    if wn is None:\n",
        "        return sentence\n",
        "    toks = sentence.split()\n",
        "    out = []\n",
        "    for t in toks:\n",
        "        if random.random() < prob:\n",
        "            syns = set()\n",
        "            for syn in wn.synsets(t):\n",
        "                for l in syn.lemmas():\n",
        "                    w = l.name().replace(\"_\", \" \")\n",
        "                    if w.lower() != t.lower():\n",
        "                        syns.add(w)\n",
        "            if syns:\n",
        "                out.append(random.choice(list(syns)))\n",
        "                continue\n",
        "        out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "# Apply augmentation: noisy Kriol source, slight synonym replacement on English target (train only)\n",
        "aug_factor = 0.2  # 20% of train set\n",
        "num_aug = int(aug_factor * len(train_df))\n",
        "_sel = train_df.sample(n=num_aug, random_state=CFG.SEED)\n",
        "_aug = _sel.copy()\n",
        "_aug[CFG.SRC_COL] = _aug[CFG.SRC_COL].map(lambda s: inject_noise(s, 0.07, 0.07))\n",
        "_aug[CFG.TGT_COL] = _aug[CFG.TGT_COL].map(lambda s: synonym_replace_en(s, 0.08))\n",
        "\n",
        "train_df = (\n",
        "    pd.concat([train_df, _aug], ignore_index=True)\n",
        "      .drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "print(f\"Augmented +{len(train_df)-len(df_filtered)} examples (train={len(train_df)}, val={len(val_df)})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7 — Generate synthetic data\n",
        "Creates English→Kriol synthetic pairs using a reverse model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Back-translation:   0%|          | 0/283 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "\n",
        "os.makedirs(os.path.dirname(CFG.SYNTH_CSV), exist_ok=True)\n",
        "\n",
        "# If CSV exists, just report and skip generation; otherwise, generate\n",
        "if os.path.exists(CFG.SYNTH_CSV):\n",
        "    print(f\"Using existing synthetic: {CFG.SYNTH_CSV}\")\n",
        "    try:\n",
        "        _samp = CFG.SYNTH_CSV.replace(\".csv\", \"_sample.csv\")\n",
        "        if os.path.exists(_samp):\n",
        "            print(\"Sample exists:\", _samp)\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    _tok = AutoTokenizer.from_pretrained(CFG.EN2KR_MODEL)\n",
        "    _mdl = AutoModelForSeq2SeqLM.from_pretrained(CFG.EN2KR_MODEL).to(device).eval()\n",
        "\n",
        "    # Collect English and filter extreme-length sentences for cleaner synthetic\n",
        "    _eng = train_df[CFG.TGT_COL].tolist()\n",
        "    _eng_all = [e for e in _eng if 4 <= len(str(e).split()) <= 40]\n",
        "    _syn = []\n",
        "    try:\n",
        "        from tqdm import tqdm as _tqdm\n",
        "        _iter = _tqdm(range(0, len(_eng_all), CFG.BT_BATCH), total=(len(_eng_all)+CFG.BT_BATCH-1)//CFG.BT_BATCH, desc=\"Back-translation\")\n",
        "    except Exception:\n",
        "        _iter = range(0, len(_eng_all), CFG.BT_BATCH)\n",
        "\n",
        "    for i in _iter:\n",
        "        chunk = _eng_all[i:i+CFG.BT_BATCH]\n",
        "        batch = _tok(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = _mdl.generate(\n",
        "            **batch,\n",
        "            max_new_tokens=CFG.GEN_MAX_NEW_TOKENS,\n",
        "            num_beams=max(3, CFG.BT_BEAM_SIZE),\n",
        "            num_return_sequences=CFG.BT_NUM_BEST,\n",
        "            length_penalty=CFG.BT_LENGTH_PENALTY,\n",
        "            early_stopping=CFG.BT_EARLY_STOPPING,\n",
        "            no_repeat_ngram_size=3,\n",
        "            repetition_penalty=CFG.BT_REPETITION_PENALTY,\n",
        "            min_length=CFG.BT_MIN_LENGTH,\n",
        "        )\n",
        "        # Select best per source by simple length/uniqueness heuristic\n",
        "        decoded = _tok.batch_decode(out, skip_special_tokens=True)\n",
        "        for j in range(0, len(decoded), CFG.BT_NUM_BEST):\n",
        "            cand = decoded[j:j+CFG.BT_NUM_BEST]\n",
        "            cand = [c for c in cand if c and len(c.split()) >= CFG.BT_MIN_LENGTH]\n",
        "            if not cand:\n",
        "                _syn.append(\"\")\n",
        "                continue\n",
        "            # prefer higher unique token ratio\n",
        "            def _u(x):\n",
        "                t = x.split()\n",
        "                return 0 if not t else len(set(t))/len(t)\n",
        "            cand.sort(key=_u, reverse=True)\n",
        "            _syn.append(cand[0])\n",
        "\n",
        "    _synth = pd.DataFrame({CFG.SRC_COL: _syn, CFG.TGT_COL: _eng_all})\n",
        "    _synth.to_csv(CFG.SYNTH_CSV, index=False)\n",
        "    try:\n",
        "        _samp = CFG.SYNTH_CSV.replace(\".csv\", \"_sample.csv\")\n",
        "        _synth.head(min(100, len(_synth))).to_csv(_samp, index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"Saved:\", CFG.SYNTH_CSV, \"rows:\", len(_synth))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8 — Integrate synthetic data\n",
        "Applies the same normalization & filtering rules as real data, dedups, and preserves real-only validation (train-only merge).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Round-trip filter (keeps rows whose back-translation matches English)\n",
        "try:\n",
        "    _final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "    if os.path.isdir(_final_dir):\n",
        "        _tok_f = AutoTokenizer.from_pretrained(_final_dir)\n",
        "        _mdl_f = AutoModelForSeq2SeqLM.from_pretrained(_final_dir).to(device).eval()\n",
        "        def _bt(texts):\n",
        "            _in = _tok_f(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "            _in = {k: v.to(device) for k, v in _in.items()}\n",
        "            _out = _mdl_f.generate(**_in, max_new_tokens=CFG.GEN_MAX_NEW_TOKENS, num_beams=4, early_stopping=True)\n",
        "            return _tok_f.batch_decode(_out, skip_special_tokens=True)\n",
        "        _cand = _sel.copy()  # pyright: ignore[reportUndefinedVariable]\n",
        "        _bt_eng = _bt(_cand[CFG.SRC_COL].tolist())\n",
        "        from sacrebleu import corpus_chrf\n",
        "        _scores = []\n",
        "        for hyp, ref in zip(_bt_eng, _cand[CFG.TGT_COL].tolist()):\n",
        "            try:\n",
        "                _scores.append(corpus_chrf([hyp], [[ref]]).score)\n",
        "            except Exception:\n",
        "                _scores.append(0.0)\n",
        "        _rt_mask = pd.Series(_scores) >= 55.0\n",
        "        _sel = _cand[_rt_mask.values]\n",
        "        print(f\"Round-trip kept {len(_sel)} / {len(_cand)}\")\n",
        "except Exception as _e:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8.1 — Minimal integration (train-only, 1:1 cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Integrated +176 synthetic pairs (train=18331, val=4539)\n"
          ]
        }
      ],
      "source": [
        "# Step 8 — Minimal integration (train-only, 1:1 cap)\n",
        "import pandas as _pd\n",
        "_syn_path = CFG.SYNTH_CSV\n",
        "if os.path.exists(_syn_path):\n",
        "    _raw = _pd.read_csv(_syn_path)[[CFG.SRC_COL, CFG.TGT_COL]].dropna()\n",
        "    _raw[CFG.SRC_COL] = _raw[CFG.SRC_COL].apply(normalize_src_text)\n",
        "    _raw[CFG.TGT_COL] = _raw[CFG.TGT_COL].apply(normalize_tgt_text)\n",
        "    _raw = _raw.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "    _raw = _raw[_raw.apply(passes_filters, axis=1)]\n",
        "\n",
        "    # Extra hygiene: drop synthetic that looks English or gibberish\n",
        "    try:\n",
        "        import langid as _lid\n",
        "    except Exception:\n",
        "        _lid = None\n",
        "\n",
        "    def _looks_english(s):\n",
        "        if _lid is None:\n",
        "            return False\n",
        "        try:\n",
        "            return _lid.classify(str(s))[0] == \"en\"\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _has_low_var(s):\n",
        "        t = str(s).split()\n",
        "        if not t:\n",
        "            return True\n",
        "        uniq = len(set(t)) / max(1, len(t))\n",
        "        if uniq < 0.3:\n",
        "            return True\n",
        "        if any(ch * 4 in s for ch in \"abcdefghijklmnopqrstuvwxyz\"):  # long char repeats\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    _raw = _raw[~_raw[CFG.SRC_COL].map(_looks_english)]\n",
        "    _raw = _raw[~_raw[CFG.SRC_COL].map(_has_low_var)]\n",
        "\n",
        "    # Relax Kriol-marker filter: allow 0 to keep diverse tpi-like sentences, but penalize low-variance\n",
        "    def _kriol_marker_score(s: str) -> int:\n",
        "        markers = {\"bin\",\"langa\",\"blanga\",\"det\",\"im\",\"imbin\",\"garra\",\"olabat\",\"nomo\",\"wal\",\"mob\",\"deya\",\"garram\",\"gin\",\"seim\"}\n",
        "        toks = str(s).lower().split()\n",
        "        return sum(1 for t in set(toks) if t in markers)\n",
        "    # keep rows with score >= 0 (no drop), but later heuristics already removed English/low-var\n",
        "\n",
        "    _cap = min(len(_raw), int(CFG.SYNTH_MAX_RATIO * len(train_df)))\n",
        "    _sel = _raw.sample(n=_cap, random_state=CFG.SEED) if _cap < len(_raw) else _raw\n",
        "\n",
        "    _before = len(train_df)\n",
        "    train_df = (\n",
        "        pd.concat([train_df, _sel], ignore_index=True)\n",
        "          .drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(f\"Integrated +{len(train_df)-_before} synthetic pairs (train={len(train_df)}, val={len(val_df)})\")\n",
        "else:\n",
        "    print(f\"Synthetic CSV not found at {_syn_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9 — Custom tokenizer training\n",
        "Trains a shared SentencePiece tokenizer on combined Kriol+English corpus when enabled. Skipped unless `CFG.USE_SPM=True` and `sentencepiece` is installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers for custom tokenizer training (SentencePiece)\n",
        "if CFG.USE_SPM:\n",
        "    try:\n",
        "        import sentencepiece as spm\n",
        "        from transformers import AlbertTokenizer\n",
        "        \n",
        "        # Prepare corpus\n",
        "        all_texts = []\n",
        "        all_texts.extend(train_df[CFG.SRC_COL].tolist() * 2)  # Weight Kriol 2x\n",
        "        all_texts.extend(train_df[CFG.TGT_COL].tolist())\n",
        "        \n",
        "        # Write corpus\n",
        "        corpus_path = os.path.join(CFG.SPM_DIR, \"corpus.txt\")\n",
        "        os.makedirs(CFG.SPM_DIR, exist_ok=True)\n",
        "        with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for text in all_texts:\n",
        "                f.write(text.strip() + \"\\n\")\n",
        "        \n",
        "        # Train SPM\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=corpus_path,\n",
        "            model_prefix=os.path.join(CFG.SPM_DIR, \"spm\"),\n",
        "            vocab_size=12000,  # Larger for better coverage\n",
        "            model_type=\"unigram\",\n",
        "            character_coverage=0.9995,\n",
        "            byte_fallback=True,\n",
        "            split_digits=False,\n",
        "            user_defined_symbols=[\"<tpi_Latn>\", \"<eng_Latn>\"]\n",
        "        )\n",
        "        \n",
        "        # Create HF-compatible tokenizer\n",
        "        tokenizer_custom = AlbertTokenizer(\n",
        "            vocab_file=os.path.join(CFG.SPM_DIR, \"spm.model\"),\n",
        "            do_lower_case=False,\n",
        "            keep_accents=True\n",
        "        )\n",
        "        tokenizer_custom.save_pretrained(CFG.SPM_DIR)\n",
        "        print(f\"Custom tokenizer saved to {CFG.SPM_DIR}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Tokenizer training failed: {e}\")\n",
        "        CFG.USE_SPM = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9.1 — Custom tokenizer (SentencePiece 10k)\n",
        "\n",
        "This cell trains a shared unigram SentencePiece tokenizer (10k vocab) over combined Kriol+English corpus. It saves to `CFG.SPM_DIR` and does not change the active tokenizer unless `CFG.USE_SPM=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9.1 — Train SPM and save Hugging Face tokenizer\n",
        "if CFG.USE_SPM:\n",
        "    try:\n",
        "        import sentencepiece as spm\n",
        "        from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "        # 1) Train SentencePiece on combined corpus\n",
        "        lines = prepare_tokenizer_corpus(pd.concat([train_df, val_df], ignore_index=True), CFG.SRC_COL, CFG.TGT_COL)\n",
        "        prefix = os.path.join(CFG.SPM_DIR, \"spm_kriol_en_v1\")\n",
        "        os.makedirs(CFG.SPM_DIR, exist_ok=True)\n",
        "        train_sentencepiece_corpus(lines, model_prefix=prefix, vocab_size=10000)\n",
        "\n",
        "        # 2) Wrap SPM into a HF fast tokenizer and add language tokens\n",
        "        spm_model_path = prefix + \".model\"\n",
        "        tokenizer_sp = PreTrainedTokenizerFast(bos_token=\"<s>\", eos_token=\"</s>\", unk_token=\"<unk>\", pad_token=\"<pad>\")\n",
        "        tokenizer_sp.backend_tokenizer = None\n",
        "        tokenizer_sp.sp_model = spm.SentencePieceProcessor()\n",
        "        tokenizer_sp.sp_model.Load(spm_model_path)\n",
        "        tokenizer_sp.add_special_tokens({\"additional_special_tokens\": [CFG.SRC_LANG, CFG.TGT_LANG]})\n",
        "\n",
        "        # 3) Save tokenizer in HF format for AutoTokenizer.from_pretrained\n",
        "        tokenizer_sp.save_pretrained(CFG.SPM_DIR)\n",
        "        print(\"SPM trained and HF tokenizer saved at:\", CFG.SPM_DIR)\n",
        "    except Exception as e:\n",
        "        print(\"SPM training/packaging failed:\", e)\n",
        "else:\n",
        "    print(\"CFG.USE_SPM is False; skipping SPM training and packaging.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Train SentencePiece tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SentencePiece tokenizer (optional)\n",
        "if CFG.USE_SPM:\n",
        "    try:\n",
        "        import sentencepiece  # noqa: F401\n",
        "        lines = prepare_tokenizer_corpus(pd.concat([train_df, val_df], ignore_index=True), CFG.SRC_COL, CFG.TGT_COL)\n",
        "        train_sentencepiece_corpus(lines, model_prefix=os.path.join(CFG.SPM_DIR, \"spm_kriol_en_v1\"), vocab_size=10000)\n",
        "        print(\"SentencePiece trained at:\", CFG.SPM_DIR)\n",
        "    except Exception as e:\n",
        "        print(\"SentencePiece training skipped/failed:\", e)\n",
        "else:\n",
        "    print(\"CFG.USE_SPM is False; skipping SentencePiece training.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10 — Tokenizer & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[warn] CFG.SRC_LANG is unset. Using fallback src_lang=eng_Latn. Set CFG.SRC_LANG explicitly when decided.\n",
            "cuda NVIDIA GeForce RTX 5060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tokenizer & Model (use custom tokenizer when CFG.USE_SPM=True)\n",
        "src_lang = CFG.SRC_LANG\n",
        "tgt_lang = CFG.TGT_LANG\n",
        "\n",
        "if CFG.USE_SPM:\n",
        "    print(\"Load tokenizer built in Step 9.1\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.SPM_DIR)\n",
        "    # Ensure language tokens exist\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": [src_lang, tgt_lang]})\n",
        "    # Load base model and resize embeddings\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(CFG.MODEL_NAME)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    # Configure decoder start to target language token\n",
        "    forced_bos = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    if hasattr(model, \"config\"):\n",
        "        model.config.use_cache = False\n",
        "        if forced_bos is not None and forced_bos != tokenizer.unk_token_id:\n",
        "            model.config.forced_bos_token_id = forced_bos\n",
        "        if getattr(model.config, \"decoder_start_token_id\", None) is None:\n",
        "            model.config.decoder_start_token_id = model.config.forced_bos_token_id\n",
        "else:\n",
        "    print(\"Default NLLB tokenizer with language tags\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(CFG.MODEL_NAME)\n",
        "    if hasattr(model, \"config\"):\n",
        "        model.config.use_cache = False\n",
        "    if hasattr(model, \"config\") and hasattr(tokenizer, \"lang_code_to_id\"):\n",
        "        forced_bos = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "        if forced_bos is not None and forced_bos != tokenizer.unk_token_id:\n",
        "            model.config.forced_bos_token_id = forced_bos\n",
        "        if getattr(model.config, \"decoder_start_token_id\", None) is None:\n",
        "            model.config.decoder_start_token_id = model.config.forced_bos_token_id\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is not available. Please check your GPU drivers and PyTorch install.\"\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(device, torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11 — Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18331, 4539)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class PairedTextDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int):\n",
        "        self.src = df[CFG.SRC_COL].tolist()\n",
        "        self.tgt = df[CFG.TGT_COL].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        src_text = str(self.src[idx])\n",
        "        tgt_text = str(self.tgt[idx])\n",
        "        model_inputs = self.tokenizer(\n",
        "            src_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        labels = self.tokenizer(\n",
        "            text_target=tgt_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
        "        item[\"labels\"] = labels[\"input_ids\"].squeeze(0)\n",
        "        return item\n",
        "\n",
        "train_ds = PairedTextDataset(train_df, tokenizer, CFG.MAX_LEN)\n",
        "val_ds = PairedTextDataset(val_df, tokenizer, CFG.MAX_LEN)\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12.1 — Trainer setup (DDP-ready) / Length-grouped sampler\n",
        "Creates `LengthGroupedSampler` to cluster sequences of similar lengths, reducing padding and stabilizing training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
        "# Curriculum-like batching: length-grouped sampler to reduce padding and stabilize training\n",
        "train_sampler = LengthGroupedSampler(lengths=[len(str(x).split()) for x in train_df[CFG.SRC_COL].tolist()],\n",
        "                                     batch_size=CFG.BATCH_SIZE,\n",
        "                                     mega_batch_mult=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12.2 — Custom Trainer class\n",
        "Defines `CleanSeq2SeqTrainer` that sanitizes inputs (drops unintended embed keys) to avoid HF arg conflicts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility: Trainer that drops unintended *_embeds keys to avoid HF arg conflicts\n",
        "from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n",
        "\n",
        "class CleanSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def _prepare_inputs(self, inputs):\n",
        "        # Sanitize at input-prep stage too\n",
        "        inputs.pop(\"decoder_inputs_embeds\", None)\n",
        "        inputs.pop(\"inputs_embeds\", None)\n",
        "        inputs.pop(\"decoder_input_ids\", None)\n",
        "        allowed = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"}\n",
        "        filtered = {k: v for k, v in inputs.items() if k in allowed}\n",
        "        return super()._prepare_inputs(filtered)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Drop any embed keys and strictly whitelist safe args for seq2seq training\n",
        "        inputs.pop(\"decoder_inputs_embeds\", None)\n",
        "        inputs.pop(\"inputs_embeds\", None)\n",
        "        inputs.pop(\"decoder_input_ids\", None)\n",
        "        allowed = {\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"}\n",
        "        filtered = {k: v for k, v in inputs.items() if k in allowed}\n",
        "        # Call model explicitly with safe kwargs to avoid decoder ids/embeds conflicts\n",
        "        outputs = model(\n",
        "            decoder_input_ids=None,\n",
        "            decoder_inputs_embeds=None,\n",
        "            use_cache=False,\n",
        "            **filtered,\n",
        "        )\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12.3 — Regularization\n",
        "Applies dropout and attention-dropout settings on the model config for regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'CFG' has no attribute 'DROPOUT'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model.config, \u001b[33m\"\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     model.config.dropout = \u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDROPOUT\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model.config, \u001b[33m\"\u001b[39m\u001b[33mactivation_dropout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      7\u001b[39m     model.config.activation_dropout = CFG.DROPOUT\n",
            "\u001b[31mAttributeError\u001b[39m: type object 'CFG' has no attribute 'DROPOUT'"
          ]
        }
      ],
      "source": [
        "# Regularization: set model-level dropouts if supported by config\n",
        "if hasattr(model, \"config\"):\n",
        "    model.config.use_cache = False\n",
        "    if hasattr(model.config, \"dropout\"):\n",
        "        model.config.dropout = CFG.DROPOUT\n",
        "    if hasattr(model.config, \"activation_dropout\"):\n",
        "        model.config.activation_dropout = CFG.DROPOUT\n",
        "    if hasattr(model.config, \"decoder_attention_dropout\"):\n",
        "        model.config.decoder_attention_dropout = CFG.ATTENTION_DROPOUT\n",
        "    if hasattr(model.config, \"encoder_attention_dropout\"):\n",
        "        model.config.encoder_attention_dropout = CFG.ATTENTION_DROPOUT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 12.4 — Trainer args and instantiation\n",
        "Builds `DataCollatorForSeq2Seq`, `TrainingArguments`, and instantiates `CleanSeq2SeqTrainer` with early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\TARIK\\AppData\\Local\\Temp\\ipykernel_33708\\1757983301.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CleanSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = CleanSeq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "label_pad_token_id = -100\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=label_pad_token_id, padding=True)\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    num_train_epochs=max(CFG.NUM_EPOCHS, 6),\n",
        "    per_device_train_batch_size=CFG.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=CFG.BATCH_SIZE,\n",
        "    learning_rate=CFG.LR,\n",
        "    warmup_steps=CFG.WARMUP_STEPS,\n",
        "    gradient_accumulation_steps=CFG.GRAD_ACCUM_STEPS,\n",
        "    label_smoothing_factor=CFG.LABEL_SMOOTHING,\n",
        "    weight_decay=CFG.WEIGHT_DECAY,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    save_total_limit=CFG.SAVE_TOTAL_LIMIT,\n",
        "    fp16=CFG.FP16,\n",
        "    report_to=CFG.REPORT_TO,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    group_by_length=True,\n",
        "\n",
        "    # Advanced optimization\n",
        "    optim=\"adamw_8bit\" if torch.cuda.is_available() else \"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine_with_restarts\",\n",
        "    warmup_ratio=0.1,\n",
        "    \n",
        "    # Better evaluation\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # Gradient optimization\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # Mixed precision\n",
        "    bf16=CFG.BF16,\n",
        "    bf16_full_eval=CFG.BF16,\n",
        "    \n",
        "    # Save best checkpoints\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        ")\n",
        "\n",
        "# Add BLEU/chrF++ as training metrics\n",
        "from datasets import load_metric\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "chrf = load_metric(\"chrf\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    bleu_score = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "    chrf_score = chrf.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "    \n",
        "    return {\n",
        "        \"bleu\": bleu_score[\"score\"],\n",
        "        \"chrf\": chrf_score[\"score\"]\n",
        "    }\n",
        "\n",
        "trainer = CleanSeq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 13 — Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3438' max='3438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3438/3438 10:44:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.909500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.292200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.939700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.811100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.525500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.432500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.236900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.111600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.093800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.938700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.921400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.927300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.906700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.890400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.815000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.852100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.821400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.688800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.729800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>1.696100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.672300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.697100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>1.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>1.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>1.644400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>1.598700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>1.633000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.612700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>1.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.605100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>1.505500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>1.502100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.518600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>1.499400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>1.533800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.516400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>1.527500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.490800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>1.507700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.528700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>1.490600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.511500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>1.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.526600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>1.526500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.511300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>1.466900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>1.506400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.469400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='568' max='568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [568/568 02:34]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_loss: 1.5678737163543701\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate once to log metrics\n",
        "metrics = trainer.evaluate()\n",
        "print(\"eval_loss:\", metrics.get(\"eval_loss\"))\n",
        "\n",
        "# Launch TensorBoard from notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"../model\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 14 — Save HF artifacts and a .pth checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved .pth to: ../model/final\\model_state.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "trainer.save_model(final_dir)\n",
        "model_path = os.path.join(final_dir, \"model_state.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Saved .pth to: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 15 — Inference helper (final only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1\n",
            "Kriol: brom deya neksdei jisas en detlot thribala wekinmen bin godan brom det hil en loda pipul bin kaman langa jisas\n",
            "English predicted: On the next day , when they came down from the mountain , a great crowd came to him . \n",
            "English original: On the next day , when they had come down from the mountain , a great multitude met him . \n",
            "-\n",
            "Sample 2\n",
            "Kriol: maitbi yumob nomo sabi ol yumob kristjan pipul yumob na det serramoni pleis blanga god en det holi spirit im jidanbat langa yumob\n",
            "English predicted: Don't you know that you are the temple of God , and of the Holy Spirit who lives in you ? \n",
            "English original: Don't you know that you are God's temple and that God's Spirit lives in you ? \n",
            "-\n",
            "Sample 3\n",
            "Kriol: if ai bina dum tharran wal ai bina prei en askim im blanga jandim jigiwan gras langa main fam en nomo larram detlot sid daga blanga gro langa main famâ lagijat na job bin tok en imbin jidan kwaitbala na\n",
            "English predicted: If I had done so , I would have prayed that he would have given me pasture , and not grain . â€\n",
            "English original: let briers grow instead of wheat , and stinkweed instead of barley . â€ The words of Job are ended . \n",
            "-\n"
          ]
        }
      ],
      "source": [
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "\n",
        "final_tok = AutoTokenizer.from_pretrained(final_dir, src_lang=(CFG.SRC_LANG or \"eng_Latn\"), tgt_lang=(CFG.TGT_LANG or \"eng_Latn\"))\n",
        "final_model = AutoModelForSeq2SeqLM.from_pretrained(final_dir).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with(model, tok, texts):\n",
        "    inputs = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    forced_bos = tok.convert_tokens_to_ids(CFG.TGT_LANG or \"eng_Latn\")\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=CFG.GEN_MAX_NEW_TOKENS,\n",
        "        num_beams=CFG.BEAM_SIZE,\n",
        "        length_penalty=CFG.LENGTH_PENALTY,\n",
        "        early_stopping=CFG.EARLY_STOPPING,\n",
        "        forced_bos_token_id=forced_bos,\n",
        "    )\n",
        "    return tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "# Pick 3 random samples from training data and show Kriol / predicted / original\n",
        "rows = val_df.sample(n=3)\n",
        "kriols = rows[CFG.SRC_COL].astype(str).tolist()\n",
        "eng_refs = rows[CFG.TGT_COL].astype(str).tolist()\n",
        "eng_preds = generate_with(final_model, final_tok, kriols)\n",
        "\n",
        "for i, (kriol, pred, ref) in enumerate(zip(kriols, eng_preds, eng_refs), start=1):\n",
        "    print(f\"Sample {i}\")\n",
        "    print(\"Kriol:\", kriol)\n",
        "    print(\"English predicted:\", pred)\n",
        "    print(\"English original:\", ref)\n",
        "    print(\"-\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 16 — COMET evaluation (final only)\n",
        "Scores validation translations with Unbabel COMET if available; otherwise prints a note (Python 3.13 may lack wheels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET prep: generating translations:   0%|          | 0/142 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "COMET prep: generating translations: 100%|██████████| 142/142 [19:38<00:00,  8.30s/it]\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\TARIK\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 5060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 142/142 [2:39:46<00:00, 67.51s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMET (final): 0.5833\n"
          ]
        }
      ],
      "source": [
        "# COMET: evaluate final only\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "\n",
        "    BATCH = CFG.COMET_BATCH\n",
        "    refs = val_df[CFG.TGT_COL].tolist()\n",
        "    srcs = val_df[CFG.SRC_COL].tolist()\n",
        "\n",
        "    # Show progress while generating hypotheses for COMET\n",
        "    try:\n",
        "        from tqdm import tqdm as _tqdm\n",
        "        _iter = _tqdm(\n",
        "            range(0, len(val_df), BATCH),\n",
        "            total=(len(val_df) + BATCH - 1) // BATCH,\n",
        "            desc=\"COMET prep: generating translations\",\n",
        "        )\n",
        "    except Exception:\n",
        "        _iter = range(0, len(val_df), BATCH)\n",
        "\n",
        "    # Faster generation for COMET (greedy, mixed precision)\n",
        "    @torch.no_grad()\n",
        "    def fast_generate(texts):\n",
        "        inputs = final_tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        forced_bos = final_tok.convert_tokens_to_ids(CFG.TGT_LANG or \"eng_Latn\")\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_cuda):\n",
        "            out = final_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=CFG.GEN_MAX_NEW_TOKENS,\n",
        "                num_beams=1,\n",
        "                do_sample=False,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "                forced_bos_token_id=forced_bos,\n",
        "            )\n",
        "        return final_tok.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "    def batched_hyps_final():\n",
        "        hyps = []\n",
        "        for i in _iter:\n",
        "            hyps.extend(fast_generate(srcs[i:i+BATCH]))\n",
        "        return hyps\n",
        "\n",
        "    hyps_final = batched_hyps_final()\n",
        "    data_final = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(srcs, hyps_final, refs)]\n",
        "\n",
        "    model_path = download_model(CFG.COMET_MODEL)\n",
        "    comet_model = load_from_checkpoint(model_path)\n",
        "\n",
        "    def get_score(output):\n",
        "        if isinstance(output, dict):\n",
        "            return output.get(\"system_score\") or output.get(\"score\") or output.get(\"mean_score\")\n",
        "        try:\n",
        "            _, s = output\n",
        "            return s\n",
        "        except Exception:\n",
        "            return output\n",
        "\n",
        "    def get_segments(output):\n",
        "        if isinstance(output, dict):\n",
        "            segs = output.get(\"segments\") or output.get(\"scores\") or output.get(\"segment_scores\")\n",
        "            if isinstance(segs, list):\n",
        "                return segs\n",
        "        return None\n",
        "\n",
        "    # Try to enable COMET's internal progress bar if supported\n",
        "    try:\n",
        "        out_final = comet_model.predict(\n",
        "            data_final,\n",
        "            batch_size=BATCH,\n",
        "            gpus=1 if torch.cuda.is_available() else 0,\n",
        "            progress_bar=True,\n",
        "        )\n",
        "    except TypeError:\n",
        "        out_final = comet_model.predict(\n",
        "            data_final,\n",
        "            batch_size=BATCH,\n",
        "            gpus=1 if torch.cuda.is_available() else 0,\n",
        "        )\n",
        "\n",
        "    sf = get_score(out_final)\n",
        "    try:\n",
        "        print(\"COMET (final):\", f\"{float(sf):.4f}\")\n",
        "    except Exception:\n",
        "        print(\"COMET (final, raw):\", sf)\n",
        "\n",
        "    # Save COMET outputs to disk\n",
        "    final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    def safe_float(x):\n",
        "        try:\n",
        "            return float(x)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Write system score\n",
        "    sf_f = safe_float(sf)\n",
        "    try:\n",
        "        with open(os.path.join(final_dir, \"system_score.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sf_f if sf_f is not None else sf}\\n\")\n",
        "    except Exception as _e:\n",
        "        print(\"Could not save final system score:\", _e)\n",
        "\n",
        "    # Write per-segment CSV (src, mt, ref, score)\n",
        "    seg_final = get_segments(out_final)\n",
        "\n",
        "    try:\n",
        "        if isinstance(seg_final, list) and len(seg_final) == len(hyps_final):\n",
        "            df_final = pd.DataFrame({\n",
        "                \"src\": srcs,\n",
        "                \"mt\": hyps_final,\n",
        "                \"ref\": refs,\n",
        "                \"comet_score\": seg_final,\n",
        "            })\n",
        "            df_final.to_csv(os.path.join(final_dir, \"comet_segments.csv\"), index=False)\n",
        "    except Exception as _e:\n",
        "        print(\"Could not save final segments CSV:\", _e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"COMET evaluation unavailable:\", e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
