{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal MarianMT Baseline (Phase 1: 3.1.1)\n",
        "\n",
        "This notebook implements a minimal Kriol → English baseline using MarianMT with its default tokenizer.\n",
        "\n",
        "- Keeps the built-in Marian tokenizer (no custom tokenizer swap in Phase 1)\n",
        "- 1-epoch fine-tune to validate end-to-end pipeline\n",
        "- Saves a `.pth` checkpoint and Hugging Face artifacts for reproducibility\n",
        "\n",
        "References:\n",
        "- Hugging Face MarianMT documentation: https://huggingface.co/docs/transformers/en/model_doc/marian\n",
        "- Example walkthrough (training & inference): https://www.kaggle.com/code/suraj520/marianmt-know-train-infer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0+cu129\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CFG:\n",
        "    MODEL_NAME = \"Helsinki-NLP/opus-mt-mul-en\"  # many → English (MarianMT)\n",
        "    DATA_FILE = \"../data/train_data.xlsx\"\n",
        "    SRC_COL = \"kriol\"\n",
        "    TGT_COL = \"english\"\n",
        "    OUTPUT_DIR = \"../model/minimal_marianmt\"\n",
        "    NUM_EPOCHS = 1\n",
        "    BATCH_SIZE = 8\n",
        "    LR = 5e-5\n",
        "    MAX_LEN = 128\n",
        "    VAL_SIZE = 0.1\n",
        "    SEED = 42\n",
        "\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(CFG.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(CFG.SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading XLSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21096 2345\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if CFG.DATA_FILE.endswith(\".csv\"):\n",
        "    df = pd.read_csv(CFG.DATA_FILE)\n",
        "elif CFG.DATA_FILE.endswith((\".xlsx\", \".xls\")):\n",
        "    df = pd.read_excel(CFG.DATA_FILE)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported data file format: use .csv or .xlsx\")\n",
        "\n",
        "assert CFG.SRC_COL in df.columns and CFG.TGT_COL in df.columns, f\"Columns not found: {CFG.SRC_COL}, {CFG.TGT_COL}\"\n",
        "\n",
        "# Basic cleaning\n",
        "df = df[[CFG.SRC_COL, CFG.TGT_COL]].dropna()\n",
        "df[CFG.SRC_COL] = df[CFG.SRC_COL].astype(str).str.strip()\n",
        "df[CFG.TGT_COL] = df[CFG.TGT_COL].astype(str).str.strip()\n",
        "df = df[(df[CFG.SRC_COL] != \"\") & (df[CFG.TGT_COL] != \"\")]\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=CFG.VAL_SIZE, random_state=CFG.SEED)\n",
        "print(len(train_df), len(val_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1.3 Data Preprocessing & Normalization (Kriol → English)\n",
        "\n",
        "This step prepares pairs for Kriol→English only:\n",
        "- Lowercase both sides; normalize whitespace (already applied earlier)\n",
        "- Deduplicate pairs to avoid train/val leakage\n",
        "- Length filter: max 128 tokens on each side\n",
        "- Length ratio filter: src/tgt and tgt/src ≤ 3.0\n",
        "- Optional: language ID filter for English targets (off by default)\n",
        "\n",
        "Note: These filters run before the split to ensure clean train/val sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filters: 20509 2279\n"
          ]
        }
      ],
      "source": [
        "# 3.1.3 Filters (no-op defaults are safe)\n",
        "APPLY_ENGLISH_LID = False  # set True to enable English LID on targets\n",
        "MAX_TOKENS = 128\n",
        "LEN_RATIO = 3.0\n",
        "\n",
        "try:\n",
        "    import langid\n",
        "except Exception:\n",
        "    langid = None\n",
        "\n",
        "\n",
        "def simple_token_count(text: str) -> int:\n",
        "    return len(str(text).split())\n",
        "\n",
        "\n",
        "def passes_filters(row) -> bool:\n",
        "    src = str(row[CFG.SRC_COL]).strip().lower()\n",
        "    tgt = str(row[CFG.TGT_COL]).strip().lower()\n",
        "    if src == \"\" or tgt == \"\":\n",
        "        return False\n",
        "    # length tokens\n",
        "    s_len = simple_token_count(src)\n",
        "    t_len = simple_token_count(tgt)\n",
        "    if s_len > MAX_TOKENS or t_len > MAX_TOKENS:\n",
        "        return False\n",
        "    # ratio\n",
        "    if s_len > 0 and t_len > 0:\n",
        "        if s_len / t_len > LEN_RATIO or t_len / s_len > LEN_RATIO:\n",
        "            return False\n",
        "    # optional English LID on target\n",
        "    if APPLY_ENGLISH_LID and langid is not None:\n",
        "        lid, _ = langid.classify(tgt)\n",
        "        if lid != \"en\":\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Apply filters before split (dedup + filters)\n",
        "df_filtered = df.copy()\n",
        "df_filtered[CFG.SRC_COL] = df_filtered[CFG.SRC_COL].astype(str).str.strip().str.lower()\n",
        "df_filtered[CFG.TGT_COL] = df_filtered[CFG.TGT_COL].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Deduplicate full pairs\n",
        "df_filtered = df_filtered.drop_duplicates(subset=[CFG.SRC_COL, CFG.TGT_COL])\n",
        "\n",
        "# Row-wise filter\n",
        "df_filtered = df_filtered[df_filtered.apply(passes_filters, axis=1)]\n",
        "\n",
        "train_df, val_df = train_test_split(df_filtered, test_size=CFG.VAL_SIZE, random_state=CFG.SEED)\n",
        "print(\"After filters:\", len(train_df), len(val_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "⚠️ DO NOT RUN NOW (enable in Phase 3.3.5 – Tokenizer Adoption)\n",
        "\n",
        "### 3.1.2 Tokenizer Training Plan (Design – no-op)\n",
        "\n",
        "- Objective: Prepare a shared SentencePiece tokenizer plan for Kriol↔English without changing current training.\n",
        "- Corpus: All training sentences from both Kriol and English sides combined.\n",
        "- Preprocessing: lowercase, normalize whitespace/punctuation, keep dialectal spellings, drop empties, deduplicate.\n",
        "- Hyperparameters: vocab_size=8000 (up to 12000 if OOV>1.5%), character_coverage=0.9995, model_type=unigram; special tokens: <pad>, <s>, </s>, <unk>.\n",
        "- Training flags (indicative):\n",
        "  - --model_type=unigram --vocab_size=8000 --character_coverage=0.9995\n",
        "  - --shuffle_input_sentence=true --max_sentence_length=2048 --num_threads=[CPU cores]\n",
        "- Artifacts: spm_kriol_en_v1.model, spm_kriol_en_v1.vocab + a JSON with training config.\n",
        "- Adoption criteria: Switch only if corpus grows >30% or OOV >1.5% and A/B shows COMET improvement.\n",
        "\n",
        "Evaluation protocol (A/B): Train/score with Marian default vs SentencePiece on same split; report COMET delta and per-segment examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1.2 Tokenizer scaffolding (no-op; does not alter training unless enabled)\n",
        "USE_SPM = False  # flip to True when adopting SentencePiece in later phase\n",
        "SPM_DIR = \"../outputs/tokenizers/spm_kriol_en_v1\"  # where spm.model/spm.vocab would live\n",
        "\n",
        "\n",
        "def prepare_tokenizer_corpus(df, src_col: str, tgt_col: str):\n",
        "    \"\"\"Return a list of cleaned lines for SPM training (lowercase, strip, dedup).\"\"\"\n",
        "    src = df[src_col].astype(str).str.lower().str.strip()\n",
        "    tgt = df[tgt_col].astype(str).str.lower().str.strip()\n",
        "    lines = pd.concat([src, tgt], ignore_index=True)\n",
        "    lines = lines[lines != \"\"].drop_duplicates()\n",
        "    return lines.tolist()\n",
        "\n",
        "\n",
        "def train_sentencepiece_corpus(lines, model_prefix: str, vocab_size: int = 8000):\n",
        "    \"\"\"Sketch only: real training will be added later. No side effects here.\"\"\"\n",
        "    # import sentencepiece as spm\n",
        "    # spm.SentencePieceTrainer.Train(\n",
        "    #     input=data_path,\n",
        "    #     model_prefix=model_prefix,\n",
        "    #     vocab_size=vocab_size,\n",
        "    #     character_coverage=0.9995,\n",
        "    #     model_type=\"unigram\",\n",
        "    # )\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_tokenizer(marian_model_name: str, use_spm: bool = USE_SPM):\n",
        "    \"\"\"Return tokenizer, preferring SPM dir if enabled, else Marian default.\"\"\"\n",
        "    if use_spm and os.path.isdir(SPM_DIR):\n",
        "        return AutoTokenizer.from_pretrained(SPM_DIR)\n",
        "    return AutoTokenizer.from_pretrained(marian_model_name)\n",
        "\n",
        "# Note: current notebook flow continues to use Marian tokenizer by default.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda NVIDIA GeForce RTX 5060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Adjust if your chosen checkpoint expects language prefixes (see Marian docs)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(CFG.MODEL_NAME)\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is not available. Please check your GPU drivers and PyTorch install.\"\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(device, torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20509, 2279)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class PairedTextDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int):\n",
        "        self.src = df[CFG.SRC_COL].tolist()\n",
        "        self.tgt = df[CFG.TGT_COL].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        src_text = str(self.src[idx])\n",
        "        tgt_text = str(self.tgt[idx])\n",
        "        model_inputs = self.tokenizer(\n",
        "            src_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                tgt_text,\n",
        "                max_length=self.max_len,\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "        item = {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
        "        item[\"labels\"] = labels[\"input_ids\"].squeeze(0)\n",
        "        return item\n",
        "\n",
        "train_ds = PairedTextDataset(train_df, tokenizer, CFG.MAX_LEN)\n",
        "val_ds = PairedTextDataset(val_df, tokenizer, CFG.MAX_LEN)\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trainer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "label_pad_token_id = -100\n",
        "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=label_pad_token_id)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    num_train_epochs=CFG.NUM_EPOCHS,\n",
        "    per_device_train_batch_size=CFG.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=CFG.BATCH_SIZE,\n",
        "    learning_rate=CFG.LR,\n",
        "    warmup_steps=500,\n",
        "    gradient_accumulation_steps=2,  # effective batch ~= 16\n",
        "    label_smoothing_factor=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    report_to=[\"tensorboard\"],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train (1 epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\data\\data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1282' max='1282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1282/1282 03:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>12.095400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.914700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.162700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.840500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.634800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.498200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.227500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.162200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.122400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>3.097000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.055800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.056800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.982700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.971300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.960400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.924800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.940100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.932900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.927600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.901400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "trainer.train()\n",
        "\n",
        "# Launch TensorBoard from notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"../model/minimal_marianmt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save HF artifacts and a .pth checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved .pth to: ../model/minimal_marianmt\\final\\model_state.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "trainer.save_model(final_dir)\n",
        "model_path = os.path.join(final_dir, \"model_state.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Saved .pth to: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple inference helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i went to the store.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Reload fine-tuned artifacts from final folder for inference\n",
        "final_dir = os.path.join(CFG.OUTPUT_DIR, \"final\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(final_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(final_dir)\n",
        "model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def translate_kriol_to_english(texts: List[str], max_new_tokens: int = 64, num_beams: int = 4) -> List[str]:\n",
        "    model.eval()\n",
        "    # For Kriol → English with Marian (mul-en), no language tags are needed\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.MAX_LEN)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=num_beams,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Example: Kriol → English (replace with a real Kriol sentence)\n",
        "translate_kriol_to_english([\"Ai bin go long shop\"], num_beams=4)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMET Evaluation (Phase 1)\n",
        "Scores validation translations with Unbabel COMET if available; otherwise prints a note (Python 3.13 may lack wheels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\TARIK\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\AI Challenge\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 143/143 [00:44<00:00,  3.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMET system score: 0.5601\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# COMET scoring on validation set\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "\n",
        "    # Generate hypotheses for val set\n",
        "    BATCH = 16\n",
        "    refs = val_df[CFG.TGT_COL].tolist()\n",
        "    hyps = []\n",
        "    srcs = val_df[CFG.SRC_COL].tolist()\n",
        "    for i in range(0, len(val_df), BATCH):\n",
        "        hyps.extend(translate_kriol_to_english(srcs[i:i+BATCH], num_beams=4))\n",
        "\n",
        "    # Prepare data for COMET (src, mt, ref)\n",
        "    data = [\n",
        "        {\"src\": s, \"mt\": h, \"ref\": r}\n",
        "        for s, h, r in zip(srcs, hyps, refs)\n",
        "    ]\n",
        "\n",
        "    # Load COMET model and score\n",
        "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    comet_model = load_from_checkpoint(model_path)\n",
        "    output = comet_model.predict(data, batch_size=16, gpus=1 if torch.cuda.is_available() else 0)\n",
        "    # Handle COMET 2.x dict output\n",
        "    if isinstance(output, dict):\n",
        "        sys_score = output.get(\"system_score\") or output.get(\"score\") or output.get(\"mean_score\")\n",
        "        seg_scores = output.get(\"segments_scores\") or output.get(\"seg_scores\")\n",
        "    else:\n",
        "        # Fallback for older APIs that may return tuple\n",
        "        try:\n",
        "            seg_scores, sys_score = output\n",
        "        except Exception:\n",
        "            sys_score = output\n",
        "            seg_scores = None\n",
        "    try:\n",
        "        print(f\"COMET system score: {float(sys_score):.4f}\")\n",
        "    except Exception:\n",
        "        print(f\"COMET system score (raw): {sys_score}\")\n",
        "except Exception as e:\n",
        "    print(\"COMET evaluation unavailable:\", e)\n",
        "    print(\"Tip: use Python < 3.13 or install prebuilt wheels, then `pip install unbabel-comet`.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
